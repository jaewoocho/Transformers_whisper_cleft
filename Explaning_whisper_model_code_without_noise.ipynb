{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyB0fe7yF4Mh"
      },
      "source": [
        "# Fine-tuning Whisper on Speech Pathology Dataset\n",
        "\n",
        "## Goal\n",
        "\n",
        "The goal of the Cleft Palate project (name TBD) at Vanderbilt DSI is to classify audio clips of patients' voices as containing hypernasality (a speech impediment) or not. The patients with hypernasality can then be recommended for speech pathology intervention. This is currently evaluated by human speech pathologists, which requires access to these medical providers. Our hope is to train a model that can classify this speech impediment for expedited patient access to a speech pathologist.\n",
        "\n",
        "Tutorial created with guidance from [\"Fine Tuning OpenAI Whisper Model for Audio Classifcation in PyTorch\"](https://www.daniweb.com/programming/computer-science/tutorials/540802/fine-tuning-openai-whisper-model-for-audio-classification-in-pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-jy3au5F4Mi"
      },
      "source": [
        "## Model\n",
        "\n",
        "We plan to use the Whisper embedings from OpenAI and train a classification model, either using Whisper with a sequence classification head or another classification LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZrXyRlPF4Mi"
      },
      "source": [
        "## Data\n",
        "\n",
        "The data in this notebook is publicly available voice recordings featuring hypernasality and control groups. In the future we hope to train our model on private patient data from Vanderbilt University Medical Center (VUMC)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCbmzVnlF4Mi"
      },
      "source": [
        "### Split Data\n",
        "\n",
        "We need to split our data into train and test sets, then save those for further experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KifWdFjsF-B6",
        "outputId": "8ba11edd-aac0-4645-890c-471aea2c3025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0a0+81ea7a4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (12.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.24.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.1.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.57.1+1.g4157f3379)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.8.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.40.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.11.17)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.39.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install datasets\n",
        "!pip install librosa\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0frbPwrsF4Mj",
        "outputId": "49925a32-25f4-46ad-bdd1-2f43a1c74065"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "import datasets\n",
        "from datasets import load_dataset, DatasetDict,  Audio\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "from transformers import WhisperModel, WhisperFeatureExtractor, AdamW\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZq6ONoYGNs3",
        "outputId": "be4ab4d3-7cf2-4c9b-df3a-581d9a6758cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/cleft_palate_choja'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UNczqVkMCGU"
      },
      "outputs": [],
      "source": [
        "data_path = \"/workspace/cleft_palate_choja/WAV_PUBLIC_SAMPLES\"\n",
        "\n",
        "train_catalog = \"/workspace/cleft_palate_choja/train.csv\"\n",
        "test_catalog = \"/workspace/cleft_palate_choja/test.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "cK7hP1fMMnZB",
        "outputId": "75b5a900-3735-47de-9b09-98e7131ffa18"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>Sampling_Rate_(Hz)</th>\n",
              "      <th>Channels</th>\n",
              "      <th>Duration_(seconds)</th>\n",
              "      <th>folder</th>\n",
              "      <th>hypernasality</th>\n",
              "      <th>original_text</th>\n",
              "      <th>OPENAI_Whisper_text</th>\n",
              "      <th>WAV_filename</th>\n",
              "      <th>WAV_folder</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ACPA ted had a dog with white feet-3.mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.13</td>\n",
              "      <td>CASES</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ted had a dog with white feet</td>\n",
              "      <td>Ted and a dog with white feet.</td>\n",
              "      <td>ACPA ted had a dog with white feet-3.wav</td>\n",
              "      <td>CASES_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cdc 4 (and then go to school).mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.41</td>\n",
              "      <td>CONTROLS</td>\n",
              "      <td>0.0</td>\n",
              "      <td>and then go to school</td>\n",
              "      <td>and then go to school.</td>\n",
              "      <td>cdc 4 (and then go to school).wav</td>\n",
              "      <td>CONTROLS_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Video 1_4 (and can I have some more material).mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.60</td>\n",
              "      <td>CONTROLS</td>\n",
              "      <td>0.0</td>\n",
              "      <td>and can I have some more material</td>\n",
              "      <td>And can I have some more material?</td>\n",
              "      <td>Video 1_4 (and can I have some more material).wav</td>\n",
              "      <td>CONTROLS_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NEW - video 2 (three times).mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.28</td>\n",
              "      <td>CONTROLS</td>\n",
              "      <td>0.0</td>\n",
              "      <td>three times</td>\n",
              "      <td>Three times.</td>\n",
              "      <td>NEW - video 2 (three times).wav</td>\n",
              "      <td>CONTROLS_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cdc 4 (and then he brushed his teeth).mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.52</td>\n",
              "      <td>CONTROLS</td>\n",
              "      <td>0.0</td>\n",
              "      <td>and then he brushed his teeth</td>\n",
              "      <td>And then he brushed his teeth.</td>\n",
              "      <td>cdc 4 (and then he brushed his teeth).wav</td>\n",
              "      <td>CONTROLS_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>video 1 (pizza bundt).mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>CONTROLS</td>\n",
              "      <td>0.0</td>\n",
              "      <td>pizza bundt</td>\n",
              "      <td>Pizza Funt!</td>\n",
              "      <td>video 1 (pizza bundt).wav</td>\n",
              "      <td>CONTROLS_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>ACPA most boys like to play football-3.mp3</td>\n",
              "      <td>48000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.31</td>\n",
              "      <td>CASES</td>\n",
              "      <td>1.0</td>\n",
              "      <td>most boys like to play football</td>\n",
              "      <td>Most boys like to play football.</td>\n",
              "      <td>ACPA most boys like to play football-3.wav</td>\n",
              "      <td>CASES_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>Facebook  (take a tire).mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.75</td>\n",
              "      <td>CASES</td>\n",
              "      <td>1.0</td>\n",
              "      <td>take a tire</td>\n",
              "      <td>See you next time!</td>\n",
              "      <td>Facebook  (take a tire).wav</td>\n",
              "      <td>CASES_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>Video 5_1 (feet).mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.04</td>\n",
              "      <td>CASES</td>\n",
              "      <td>1.0</td>\n",
              "      <td>feet</td>\n",
              "      <td>Peace.</td>\n",
              "      <td>Video 5_1 (feet).wav</td>\n",
              "      <td>CASES_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>Video 1_7 (here_s some pizza).mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.12</td>\n",
              "      <td>CONTROLS</td>\n",
              "      <td>0.0</td>\n",
              "      <td>here's some pizza</td>\n",
              "      <td>Here's your pizza.</td>\n",
              "      <td>Video 1_7 (here_s some pizza).wav</td>\n",
              "      <td>CONTROLS_WAV</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>147 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             File_Name  Sampling_Rate_(Hz)  \\\n",
              "0             ACPA ted had a dog with white feet-3.mp3             44100.0   \n",
              "1                    cdc 4 (and then go to school).mp3             44100.0   \n",
              "2    Video 1_4 (and can I have some more material).mp3             44100.0   \n",
              "3                      NEW - video 2 (three times).mp3             44100.0   \n",
              "4            cdc 4 (and then he brushed his teeth).mp3             44100.0   \n",
              "..                                                 ...                 ...   \n",
              "142                          video 1 (pizza bundt).mp3             44100.0   \n",
              "143         ACPA most boys like to play football-3.mp3             48000.0   \n",
              "144                        Facebook  (take a tire).mp3             44100.0   \n",
              "145                               Video 5_1 (feet).mp3             44100.0   \n",
              "146                  Video 1_7 (here_s some pizza).mp3             44100.0   \n",
              "\n",
              "     Channels  Duration_(seconds)    folder  hypernasality  \\\n",
              "0         1.0                4.13     CASES            1.0   \n",
              "1         2.0                1.41  CONTROLS            0.0   \n",
              "2         2.0                3.60  CONTROLS            0.0   \n",
              "3         2.0                1.28  CONTROLS            0.0   \n",
              "4         2.0                1.52  CONTROLS            0.0   \n",
              "..        ...                 ...       ...            ...   \n",
              "142       2.0                1.80  CONTROLS            0.0   \n",
              "143       1.0                3.31     CASES            1.0   \n",
              "144       1.0                1.75     CASES            1.0   \n",
              "145       2.0                1.04     CASES            1.0   \n",
              "146       2.0                2.12  CONTROLS            0.0   \n",
              "\n",
              "                         original_text                  OPENAI_Whisper_text  \\\n",
              "0        ted had a dog with white feet      Ted and a dog with white feet.    \n",
              "1                and then go to school              and then go to school.    \n",
              "2    and can I have some more material  And can I have some more material?    \n",
              "3                          three times                        Three times.    \n",
              "4        and then he brushed his teeth      And then he brushed his teeth.    \n",
              "..                                 ...                                  ...   \n",
              "142                        pizza bundt                         Pizza Funt!    \n",
              "143    most boys like to play football    Most boys like to play football.    \n",
              "144                        take a tire                  See you next time!    \n",
              "145                               feet                              Peace.    \n",
              "146                  here's some pizza                  Here's your pizza.    \n",
              "\n",
              "                                          WAV_filename    WAV_folder  \n",
              "0             ACPA ted had a dog with white feet-3.wav     CASES_WAV  \n",
              "1                    cdc 4 (and then go to school).wav  CONTROLS_WAV  \n",
              "2    Video 1_4 (and can I have some more material).wav  CONTROLS_WAV  \n",
              "3                      NEW - video 2 (three times).wav  CONTROLS_WAV  \n",
              "4            cdc 4 (and then he brushed his teeth).wav  CONTROLS_WAV  \n",
              "..                                                 ...           ...  \n",
              "142                          video 1 (pizza bundt).wav  CONTROLS_WAV  \n",
              "143         ACPA most boys like to play football-3.wav     CASES_WAV  \n",
              "144                        Facebook  (take a tire).wav     CASES_WAV  \n",
              "145                               Video 5_1 (feet).wav     CASES_WAV  \n",
              "146                  Video 1_7 (here_s some pizza).wav  CONTROLS_WAV  \n",
              "\n",
              "[147 rows x 10 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_metadata = pd.read_csv(train_catalog)\n",
        "train_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nK-GGDSGnwF"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into training and validation sets: 70% for training and 30% for validation.\n",
        "# The 'random_state' ensures the split is reproducible.\n",
        "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asMFxjOFShao"
      },
      "outputs": [],
      "source": [
        "# Extracting the filenames of the WAV audio files from the training dataframe and converting them to a list.\n",
        "train_files = train_df[\"WAV_filename\"].tolist()\n",
        "\n",
        "# Extracting the folder names where the WAV audio files are stored from the training dataframe and converting them to a list.\n",
        "train_folder = train_df[\"WAV_folder\"].tolist()\n",
        "\n",
        "# Creating full file paths for each WAV file in the training set by joining the base data path, folder name, and file name.\n",
        "# This is done for all files in the 'train_files' list.\n",
        "train_full_paths = [os.path.join(data_path, train_folder[i], train_files[i]) for i in range(0, len(train_files))]\n",
        "\n",
        "# 'train_full_paths' now contains the complete paths for each audio file in the training dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H7EPlZcTkvs",
        "outputId": "34b27042-68c4-4f9a-fff4-1a4bfbde6a78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extracting the 'hypernasality' labels from the training dataframe and converting them into a list.\n",
        "train_labels = train_df[\"hypernasality\"].tolist()\n",
        "\n",
        "# Displaying the first 10 labels from the 'train_labels' list for a quick check or overview.\n",
        "train_labels[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR_oKNmxHVB5"
      },
      "outputs": [],
      "source": [
        "# Preparing the validation set:\n",
        "\n",
        "# Extracting the filenames of WAV audio files from the validation dataframe and converting them into a list.\n",
        "val_files = val_df[\"WAV_filename\"].tolist()\n",
        "\n",
        "# Extracting the folder names where the WAV audio files are stored from the validation dataframe and converting them into a list.\n",
        "val_folder = val_df[\"WAV_folder\"].tolist()\n",
        "\n",
        "# Creating full file paths for each WAV file in the validation set by joining the base data path, folder name, and file name.\n",
        "# This is performed for all files in the 'val_files' list.\n",
        "val_full_paths = [os.path.join(data_path, val_folder[i], val_files[i]) for i in range(0, len(val_files))]\n",
        "\n",
        "# Extracting the 'hypernasality' labels from the validation dataframe and converting them into a list.\n",
        "val_labels = val_df[\"hypernasality\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nVArYbmHfuV",
        "outputId": "950e7737-6bee-4e24-b24c-8c243b1e3101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Determining the total number of labels in the validation set.\n",
        "# This represents the count of samples in the validation dataset.\n",
        "len(val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSZZHxQcUmTi"
      },
      "outputs": [],
      "source": [
        "test_metadata = pd.read_csv(test_catalog)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2dxxV6vUsO2",
        "outputId": "3dc81a6a-fd3e-47da-d186-dc08b9a7348e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_968/3166376184.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n"
          ]
        }
      ],
      "source": [
        "# Adding columns to the test dataset for WAV audio file data.\n",
        "\n",
        "# In the 'WAV_filename' column, change the file extension from \".mp3\" to \".wav\" in each filename.\n",
        "# This is done by replacing '.mp3' with '.wav' in the 'File_Name' column of the test_metadata dataframe.\n",
        "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
        "\n",
        "# Create a new column 'WAV_folder' in the test_metadata dataframe.\n",
        "# This column is generated by appending \"_WAV\" to each value in the existing 'folder' column.\n",
        "# This helps in categorizing or identifying the folder as containing WAV files.\n",
        "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg--b6ufULkK"
      },
      "outputs": [],
      "source": [
        "# Extracting and preparing file path information for the test dataset:\n",
        "\n",
        "# Retrieving the filenames of WAV audio files from the test metadata dataframe and converting them into a list.\n",
        "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
        "\n",
        "# Extracting the folder names where the WAV audio files are stored in the test dataset and converting them into a list.\n",
        "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
        "\n",
        "# Generating full file paths for each WAV file in the test dataset.\n",
        "# This is achieved by joining the base data path, folder name, and file name for each file in 'test_files'.\n",
        "# The os.path.join function ensures that the paths are correctly formed irrespective of the operating system.\n",
        "test_full_paths = [os.path.join(data_path, test_folder[i], test_files[i]) for i in range(0, len(test_files))]\n",
        "\n",
        "# 'test_full_paths' now contains the complete paths for each audio file in the test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrzkUkYXVJbT"
      },
      "outputs": [],
      "source": [
        "# Extracting the 'hypernasality' labels from the test metadata dataframe and converting them into a list.\n",
        "# This list, 'test_labels', will contain the hypernasality status (likely as categorical data) for each test sample.\n",
        "test_labels = test_metadata[\"hypernasality\"].tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSk_DTreF4Ml"
      },
      "source": [
        "### Create PyTorch datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQs-OauiF4Ml"
      },
      "outputs": [],
      "source": [
        "# Creating a training dataset for audio processing:\n",
        "# 'datasets.Dataset.from_dict' creates a dataset from a dictionary.\n",
        "# Here, the dictionary has two keys: 'audio' and 'labels'.\n",
        "# 'audio' key contains the list of paths to the training audio files ('train_full_paths').\n",
        "# 'labels' key contains the corresponding labels from 'train_labels'.\n",
        "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths, \"labels\": train_labels})\n",
        "\n",
        "# Casting the 'audio' column of the dataset to a specific data type.\n",
        "# 'Audio(sampling_rate=16_000)' specifies that the data in the 'audio' column\n",
        "# should be treated as audio data with a sampling rate of 16,000 Hz.\n",
        "# This is important for ensuring consistency in audio data processing.\n",
        "train_audio_dataset = train_audio_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "\n",
        "# Creating a test dataset in a similar manner to the training dataset.\n",
        "# 'test_full_paths' provides the paths to the test audio files, and 'test_labels' are their corresponding labels.\n",
        "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths, \"labels\": test_labels})\n",
        "\n",
        "# Casting the 'audio' column in the test dataset to the Audio data type with a sampling rate of 16,000 Hz.\n",
        "test_audio_dataset = test_audio_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "\n",
        "# Creating a validation dataset, following the same procedure as for the training and test datasets.\n",
        "# 'val_full_paths' and 'val_labels' provide the audio file paths and labels for the validation data, respectively.\n",
        "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths, \"labels\": val_labels})\n",
        "\n",
        "# Casting the 'audio' column in the validation dataset to the Audio data type with the specified sampling rate.\n",
        "val_audio_dataset = val_audio_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC0SCQsZF4Mm"
      },
      "outputs": [],
      "source": [
        "# Define the model checkpoint to be used. Here, it is \"openai/whisper-base\",\n",
        "# which likely refers to a base version of OpenAI's Whisper model.\n",
        "model_checkpoint = \"openai/whisper-base\"\n",
        "\n",
        "# Initialize a feature extractor for the Whisper model.\n",
        "# The feature extractor is loaded with the weights from the specified model checkpoint.\n",
        "# Feature extractors are used to preprocess the audio data before feeding it to the model.\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Initialize the encoder part of the Whisper model.\n",
        "# The encoder is loaded with the weights from the specified model checkpoint.\n",
        "# This encoder will be used to process the extracted features and generate embeddings or predictions.\n",
        "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Setting up the device for model computations.\n",
        "# This line checks if CUDA (an interface for working with Nvidia GPUs) is available.\n",
        "# If CUDA is available, it sets the device to 'cuda' (GPU) for faster computation.\n",
        "# If not, it uses 'cpu'. Using a GPU can significantly speed up model training and inference.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09pDF62uF4Mm"
      },
      "outputs": [],
      "source": [
        "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_data,  text_processor):\n",
        "        # Constructor for the dataset class.\n",
        "        # 'audio_data' is expected to be a list or similar collection of audio data and associated metadata.\n",
        "        # 'text_processor' is a text processing tool, possibly for feature extraction or preprocessing.\n",
        "\n",
        "        self.audio_data = audio_data\n",
        "        self.text_processor = text_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        # This method returns the total number of samples in the dataset.\n",
        "        return len(self.audio_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # This method retrieves a single data sample from the dataset.\n",
        "\n",
        "        # Process the audio data at the specified index using the text_processor.\n",
        "        # 'return_tensors=\"pt\"' indicates that the processed output should be PyTorch tensors.\n",
        "        # The sampling rate from the audio data is also used in the processing.\n",
        "        inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
        "                                     return_tensors=\"pt\",\n",
        "                                     sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
        "\n",
        "        # Extract the input features from the processed inputs.\n",
        "        input_features = inputs.input_features\n",
        "\n",
        "        # Prepare the 'decoder_input_ids' which might be used by the model for decoding/processing.\n",
        "        # The 'encoder.config.decoder_start_token_id' is multiplied to create a tensor.\n",
        "        # This is model-specific and depends on how the model expects the input.\n",
        "        decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
        "\n",
        "        # Extract the label for the current audio sample.\n",
        "        # 'labels' here are likely to be categorical values or similar, stored as NumPy arrays.\n",
        "        labels = np.array(self.audio_data[index]['labels'])\n",
        "\n",
        "        # The method returns the processed input features, decoder input IDs, and the labels for the current sample.\n",
        "        return input_features, decoder_input_ids, torch.tensor(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gw-7620F4Mm"
      },
      "outputs": [],
      "source": [
        "# Create instances of the SpeechClassificationDataset for each data set (training, testing, and validation).\n",
        "# The datasets are initialized with their respective audio datasets and the feature extractor.\n",
        "train_dataset = SpeechClassificationDataset(train_audio_dataset, feature_extractor)\n",
        "test_dataset = SpeechClassificationDataset(test_audio_dataset, feature_extractor)\n",
        "val_dataset = SpeechClassificationDataset(val_audio_dataset, feature_extractor)\n",
        "\n",
        "# Define the batch size. This is the number of samples that will be processed together in one pass (batch) during training.\n",
        "batch_size = 8\n",
        "\n",
        "# Initialize a DataLoader for the training dataset.\n",
        "# 'batch_size=batch_size' configures the loader to provide data in batches of the specified size.\n",
        "# 'shuffle=True' ensures that the data is shuffled at each epoch, which helps in reducing overfitting and improving model generalization.\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize a DataLoader for the validation dataset.\n",
        "# Data shuffling is turned off ('shuffle=False') as it is not necessary for validation and testing datasets.\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize a DataLoader for the test dataset, also with shuffling turned off.\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzARqHUEF4Mm"
      },
      "source": [
        "## Fine Tune Whisper Model\n",
        "\n",
        "Whisper model from HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVjXzUnbF4Mm"
      },
      "outputs": [],
      "source": [
        "class SpeechClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, encoder):\n",
        "        # This is the constructor for the SpeechClassifier class.\n",
        "        # It initializes the class with a specific number of labels and a pre-trained encoder.\n",
        "        super(SpeechClassifier, self).__init__()\n",
        "\n",
        "        self.encoder = encoder  # This assigns the pre-trained encoder (Whisper model) to the class.\n",
        "\n",
        "        # A classifier is defined as a sequential neural network.\n",
        "        # It consists of several linear layers (fully connected layers) with ReLU activation functions in between.\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.encoder.config.hidden_size, 4096),  # Linear layer from hidden size of encoder to 4096 nodes.\n",
        "            nn.ReLU(),  # ReLU activation function.\n",
        "            nn.Linear(4096, 2048),  # Another linear layer reducing nodes from 4096 to 2048.\n",
        "            nn.ReLU(),  # ReLU activation function.\n",
        "            nn.Linear(2048, 1024),  # Linear layer from 2048 to 1024 nodes.\n",
        "            nn.ReLU(),  # ReLU activation function.\n",
        "            nn.Linear(1024, 512),  # Linear layer from 1024 to 512 nodes.\n",
        "            nn.ReLU(),  # ReLU activation function.\n",
        "            nn.Linear(512, num_labels)  # Final linear layer outputting to the number of labels.\n",
        "        )\n",
        "\n",
        "    def forward(self, input_features, decoder_input_ids):\n",
        "        # The forward method defines how the input data passes through the network.\n",
        "\n",
        "        # The encoder takes input features and decoder input IDs, returning the model's outputs.\n",
        "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
        "\n",
        "        # Extracts the last hidden state's first token representation.\n",
        "        # This is a common practice for classification tasks using transformers.\n",
        "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
        "\n",
        "        # Passes this pooled output through the classifier to get the final logits.\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIkVorEXF4Mn",
        "outputId": "855bc5f2-9fb9-4d7c-f56e-2bdf3bfa242b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Setting the number of labels (classes) for the classification task.\n",
        "# 'num_labels = 2' implies that the task is binary classification.\n",
        "num_labels = 2\n",
        "\n",
        "# Instantiating the SpeechClassifier model.\n",
        "# The model is initialized with the number of labels and the encoder (from the Whisper model).\n",
        "# The '.to(device)' method moves the model to a GPU if available, otherwise to the CPU.\n",
        "# This helps in leveraging GPU acceleration for faster computation during training.\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "\n",
        "# Initializing the optimizer for the model training.\n",
        "# 'AdamW' is a variant of the Adam optimizer, commonly used in deep learning.\n",
        "# It takes model parameters as its first argument.\n",
        "# 'lr=2e-5' sets the learning rate. This value is a common default for fine-tuning models in NLP.\n",
        "# 'betas=(0.9, 0.999)' sets the coefficients used for computing running averages of the gradient and its square.\n",
        "# 'eps=1e-08' is a very small number to prevent any division by zero in the implementation.\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
        "\n",
        "# Defining the loss function.\n",
        "# 'nn.CrossEntropyLoss' is commonly used for classification tasks.\n",
        "# It combines a SoftMax activation with a cross-entropy loss function.\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uioy2R1V1Gn"
      },
      "outputs": [],
      "source": [
        "# Define the training function for the model without a separate validation phase.\n",
        "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
        "    # 'num_epochs' is the number of times the entire training dataset is passed through the model.\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Iterating over each epoch.\n",
        "\n",
        "        model.train()\n",
        "        # Setting the model to training mode. This is crucial as it enables\n",
        "        # the training-specific operations like dropout.\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            # Looping over each batch in the training data loader.\n",
        "\n",
        "            # Unpacking the batch to get input features, decoder input IDs, and labels.\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "            # Removing unnecessary dimensions and moving the data to the specified device (CPU or GPU).\n",
        "            input_features = input_features.squeeze().to(device)\n",
        "            decoder_input_ids = decoder_input_ids.squeeze().to(device)\n",
        "\n",
        "            # Reshaping the labels, converting them to long datatype, and moving to the specified device.\n",
        "            labels = labels.view(-1).type(torch.LongTensor).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Clearing the gradients of all optimized tensors. This is important as gradients are accumulated.\n",
        "\n",
        "            # Forward pass: computing logits by passing the input features and decoder input IDs through the model.\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "            # Calculating the loss between the model outputs (logits) and the labels.\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            # Backward pass: computing gradient of the loss with respect to model parameters.\n",
        "\n",
        "            optimizer.step()\n",
        "            # Adjusting the model parameters based on the computed gradients.\n",
        "\n",
        "            # Print training progress information every 8 batches.\n",
        "            if (i+1) % 8 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Saving the model's state after each epoch. This saves the trained weights.\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziIJ-x0BF4Mn"
      },
      "outputs": [],
      "source": [
        "# Define the training function with an additional validation phase for evaluating model performance.\n",
        "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
        "\n",
        "    best_accuracy = 0.0  # Initialize the best accuracy variable to keep track of the highest accuracy reached.\n",
        "\n",
        "    for epoch in range(num_epochs):  # Iterate over each epoch.\n",
        "\n",
        "        model.train()  # Set the model to training mode.\n",
        "\n",
        "        for i, batch in enumerate(train_loader):  # Iterate over each batch in the training data loader.\n",
        "\n",
        "            # Extract input features, decoder input IDs, and labels from the current batch.\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "            # Preprocess and move the data to the appropriate device (CPU/GPU).\n",
        "            input_features = input_features.squeeze().to(device)\n",
        "            decoder_input_ids = decoder_input_ids.squeeze().to(device)\n",
        "            labels = labels.view(-1).type(torch.LongTensor).to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients for the optimizer.\n",
        "\n",
        "            # Forward pass: compute logits by passing inputs through the model.\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "            # Compute the loss between model predictions and actual labels.\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()  # Backward pass to compute gradients.\n",
        "\n",
        "            optimizer.step()  # Update model parameters based on gradients.\n",
        "\n",
        "            # Print training loss every 8 batches for monitoring.\n",
        "            if (i + 1) % 8 == 0:\n",
        "                print(f'Epoch {epoch + 1}/{num_epochs}, Batch {i + 1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Perform evaluation on the validation set after each training epoch.\n",
        "        val_loss, val_accuracy, val_f1, _, _ = evaluate(model, val_loader, device)\n",
        "\n",
        "        # Check if the current validation accuracy is the best and save the model if it is.\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pt')  # Save the model.\n",
        "\n",
        "        # Print validation performance metrics for the current epoch.\n",
        "        print(\"========================================================================================\")\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
        "        print(\"========================================================================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7Q4-ze0F4Mn"
      },
      "outputs": [],
      "source": [
        "# Define the evaluation function for the model.\n",
        "def evaluate(model, data_loader, device):\n",
        "\n",
        "    all_labels = []  # Initialize a list to store all actual labels.\n",
        "    all_preds = []   # Initialize a list to store all model predictions.\n",
        "    total_loss = 0.0  # Initialize the total loss to zero.\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for evaluation, which reduces memory usage and speeds up computation.\n",
        "\n",
        "        for i, batch in enumerate(data_loader):  # Iterate over each batch in the data loader.\n",
        "\n",
        "            # Unpack the batch to get input features, decoder input IDs, and labels.\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "            # Preprocess and move the data to the appropriate device (CPU/GPU).\n",
        "            input_features = input_features.squeeze().to(device)\n",
        "            decoder_input_ids = decoder_input_ids.squeeze().to(device)\n",
        "            labels = labels.view(-1).type(torch.LongTensor).to(device)\n",
        "\n",
        "            # Perform a forward pass through the model to get logits.\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "            # Compute the loss between the model's logits and the actual labels.\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()  # Accumulate the total loss.\n",
        "\n",
        "            # Get the class predictions from the logits.\n",
        "            _, preds = torch.max(logits, 1)\n",
        "\n",
        "            # Store the labels and predictions to calculate metrics later.\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "\n",
        "    # Concatenate all batches' labels and predictions into single arrays.\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "\n",
        "    # Calculate the average loss, accuracy, and F1 score.\n",
        "    loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    # Return the calculated metrics along with all labels and predictions.\n",
        "    return loss, accuracy, f1, all_labels, all_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUv6LcFNF4Mn",
        "outputId": "f92a0109-8d1a-4cf2-89e7-cbf354b1d859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Batch 8/13, Train Loss: 0.6367\n",
            "========================================================================================\n",
            "Epoch 1/5, Val Loss: 0.6483, Val Accuracy: 0.5111, Val F1: 0.3382, Best Accuracy: 0.5111\n",
            "========================================================================================\n",
            "Epoch 2/5, Batch 8/13, Train Loss: 0.3895\n",
            "========================================================================================\n",
            "Epoch 2/5, Val Loss: 0.3521, Val Accuracy: 0.8444, Val F1: 0.8432, Best Accuracy: 0.8444\n",
            "========================================================================================\n",
            "Epoch 3/5, Batch 8/13, Train Loss: 0.0461\n",
            "========================================================================================\n",
            "Epoch 3/5, Val Loss: 0.5403, Val Accuracy: 0.8222, Val F1: 0.8148, Best Accuracy: 0.8444\n",
            "========================================================================================\n",
            "Epoch 4/5, Batch 8/13, Train Loss: 0.0011\n",
            "========================================================================================\n",
            "Epoch 4/5, Val Loss: 0.4466, Val Accuracy: 0.8889, Val F1: 0.8889, Best Accuracy: 0.8889\n",
            "========================================================================================\n",
            "Epoch 5/5, Batch 8/13, Train Loss: 0.0007\n",
            "========================================================================================\n",
            "Epoch 5/5, Val Loss: 0.9317, Val Accuracy: 0.8000, Val F1: 0.7935, Best Accuracy: 0.8889\n",
            "========================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Importing the librosa library, commonly used for audio processing tasks.\n",
        "import librosa\n",
        "\n",
        "# Setting the number of epochs for training the model.\n",
        "# An epoch is a full iteration over the entire training dataset.\n",
        "num_epochs = 5\n",
        "\n",
        "# Initiating the training process of the model.\n",
        "# The 'train' function takes the model, train and validation data loaders, optimizer, loss criterion,\n",
        "# computation device, and number of epochs as parameters.\n",
        "# This function will train the model on the training dataset for the specified number of epochs\n",
        "# and evaluate its performance on the validation dataset after each epoch.\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjETS9sqYRD5"
      },
      "source": [
        "### Validation\n",
        "\n",
        "Before running the model on the test set, let's examine the validation set and see how our model is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bILZRSn4JvlG"
      },
      "outputs": [],
      "source": [
        "# Loading the state dictionary of the best-performing model from the saved file 'best_model.pt'.\n",
        "# This file contains the trained weights of the model that achieved the highest accuracy during training.\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Creating a new instance of the SpeechClassifier model.\n",
        "# 'num_labels' specifies the number of output labels for the classification task.\n",
        "# The model is again initialized with the same encoder as used during training.\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "\n",
        "# Loading the saved state dictionary into this new model instance.\n",
        "# This effectively transfers the learned weights to the new model.\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Evaluating the model on the validation dataset.\n",
        "# The 'evaluate' function returns multiple metrics but here we're only interested in the actual and predicted labels.\n",
        "# These labels are used to assess the model's classification performance on the validation set.\n",
        "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyNUpLHeJ5p1",
        "outputId": "52379010-0e14-4cd5-eaa7-bd858a033532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.91      0.89        22\n",
            "           1       0.91      0.87      0.89        23\n",
            "\n",
            "    accuracy                           0.89        45\n",
            "   macro avg       0.89      0.89      0.89        45\n",
            "weighted avg       0.89      0.89      0.89        45\n",
            "\n",
            "0.8888888888888888\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION PHASE CONTINUED\n",
        "\n",
        "# Printing a detailed classification report.\n",
        "# The 'classification_report' from scikit-learn provides metrics such as precision, recall, and F1-score for each class.\n",
        "# These metrics give a comprehensive view of the model's performance across different classes.\n",
        "# 'all_labels' are the true labels, and 'all_preds' are the predictions made by the model on the validation set.\n",
        "print(classification_report(all_labels, all_preds))\n",
        "\n",
        "# Printing the overall accuracy of the model on the validation set.\n",
        "# 'accuracy_score' computes the proportion of correctly predicted observations to the total observations.\n",
        "# This gives a quick and clear indication of how often the model is correct across all classes.\n",
        "print(accuracy_score(all_labels, all_preds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l56wgPY1Qja3"
      },
      "source": [
        "This is too good to be true. Checking the contents of labels, preds, and data balance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nefkmyAVLDRO",
        "outputId": "00198469-6838-4cdd-a6bb-5557a59470a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
              "       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "       1])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL-CcSn0LGgk",
        "outputId": "8c2e1758-da55-4632-f796-9246548c4e2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6GfSmERLMA4",
        "outputId": "7bedfc59-523c-4fab-943f-8bdead50feac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5294117647058824"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculating the proportion of positive labels in the training dataset.\n",
        "# This operation sums up all the values in 'train_labels' and then divides by the total number of labels.\n",
        "# 'train_labels' is a list containing the labels for each training sample.\n",
        "# Assuming a binary classification task, this line effectively computes the fraction of samples\n",
        "# that belong to the positive class (often denoted as '1').\n",
        "# This metric is useful for understanding the class distribution in the training set,\n",
        "# particularly for identifying class imbalances.\n",
        "sum(train_labels) / len(train_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiaKvCpnLRrd",
        "outputId": "5b48efc7-21b5-4ea5-d180-ce46a74bb6fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5111111111111111"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculating the proportion of positive labels in the validation dataset.\n",
        "# This line sums all the values in 'val_labels' and divides by the total number of labels.\n",
        "# 'val_labels' contains the labels for each sample in the validation dataset.\n",
        "# In a binary classification context, this calculation reveals the fraction of samples\n",
        "# that are categorized as the positive class (usually represented as '1').\n",
        "# It's a useful metric to assess the class balance in the validation set,\n",
        "# which can influence how well the model generalizes from training to validation data.\n",
        "sum(val_labels) / len(val_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBh8OAXvR7Qr",
        "outputId": "eff1a373-ef17-4a1d-9a8c-a09274f8f9a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86        18\n",
            "           1       0.89      0.84      0.86        19\n",
            "\n",
            "    accuracy                           0.86        37\n",
            "   macro avg       0.87      0.87      0.86        37\n",
            "weighted avg       0.87      0.86      0.86        37\n",
            "\n",
            "0.8648648648648649\n"
          ]
        }
      ],
      "source": [
        "# TESTING PHASE\n",
        "# This section is dedicated to evaluating the model's performance on the test dataset.\n",
        "\n",
        "# Loading the state dictionary of the best model from 'best_model.pt'.\n",
        "# This file contains the trained model weights that achieved the highest accuracy during the training phase.\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Creating a new instance of the SpeechClassifier model.\n",
        "# 'num_labels = 2' indicates that the classification task is binary.\n",
        "# The model is initialized with the specified encoder and set to the appropriate device (CPU or GPU).\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "\n",
        "# Loading the saved state dictionary into the newly created model instance.\n",
        "# This ensures that the model uses the previously trained and optimized weights.\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Evaluating the model's performance on the test dataset.\n",
        "# The 'evaluate' function is called with the test data loader and the computation device.\n",
        "# It returns the loss, accuracy, F1 score, and the actual and predicted labels, but here\n",
        "# we're only interested in the actual and predicted labels.\n",
        "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
        "\n",
        "# Printing a detailed classification report.\n",
        "# This includes metrics such as precision, recall, and F1-score for each class,\n",
        "# offering a comprehensive view of the model's performance on the test data.\n",
        "print(classification_report(all_labels, all_preds))\n",
        "\n",
        "# Printing the overall accuracy of the model on the test set.\n",
        "# This is the proportion of correctly predicted observations to the total number of observations,\n",
        "# providing a quick overview of the model's effectiveness in making correct predictions.\n",
        "print(accuracy_score(all_labels, all_preds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d_MMju6R71D"
      },
      "source": [
        "I don't want to run testing yet as we want to explore more models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quoc3-z4YaJx"
      },
      "source": [
        "### Model Troubleshooting\n",
        "\n",
        "So far our results look too good to be true (98% validation accuracy). In the cells below I run through some troubleshooting methods to ensure our model is not overfit or learning the wrong representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JoYDBxOQzJt"
      },
      "source": [
        "Ensure that the labels are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDwMkIKvNLCv",
        "outputId": "762468da-693b-40a1-9e4e-9b02213b073f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "93     0.0\n",
              "140    0.0\n",
              "108    0.0\n",
              "65     0.0\n",
              "28     0.0\n",
              "117    0.0\n",
              "84     0.0\n",
              "142    0.0\n",
              "44     0.0\n",
              "15     0.0\n",
              "114    0.0\n",
              "47     0.0\n",
              "110    0.0\n",
              "78     0.0\n",
              "5      0.0\n",
              "120    0.0\n",
              "77     0.0\n",
              "34     0.0\n",
              "111    0.0\n",
              "43     0.0\n",
              "95     0.0\n",
              "131    0.0\n",
              "8      0.0\n",
              "13     0.0\n",
              "3      0.0\n",
              "38     0.0\n",
              "72     0.0\n",
              "6      0.0\n",
              "109    0.0\n",
              "2      0.0\n",
              "123    0.0\n",
              "112    0.0\n",
              "46     0.0\n",
              "79     0.0\n",
              "41     0.0\n",
              "90     0.0\n",
              "75     0.0\n",
              "32     0.0\n",
              "141    0.0\n",
              "37     0.0\n",
              "1      0.0\n",
              "52     0.0\n",
              "103    0.0\n",
              "74     0.0\n",
              "121    0.0\n",
              "146    0.0\n",
              "20     0.0\n",
              "14     0.0\n",
              "Name: hypernasality, dtype: float64"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[train_df[\"WAV_folder\"] == \"CONTROLS_WAV\"][\"hypernasality\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "SnNHVlCdNafl",
        "outputId": "17e73924-6d48-4490-9561-875e9fb01440"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>Sampling_Rate_(Hz)</th>\n",
              "      <th>Channels</th>\n",
              "      <th>Duration_(seconds)</th>\n",
              "      <th>folder</th>\n",
              "      <th>hypernasality</th>\n",
              "      <th>original_text</th>\n",
              "      <th>OPENAI_Whisper_text</th>\n",
              "      <th>WAV_filename</th>\n",
              "      <th>WAV_folder</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>ACPA Santa came home since the snow fell.mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.19</td>\n",
              "      <td>CONTROLS</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Santa came home since the snow fell</td>\n",
              "      <td>Santa came home since the snow fell.</td>\n",
              "      <td>ACPA Santa came home since the snow fell.wav</td>\n",
              "      <td>CONTROLS_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>cdc 5 (can I play with Jack).mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.57</td>\n",
              "      <td>CONTROLS</td>\n",
              "      <td>0.0</td>\n",
              "      <td>can I play with Jack</td>\n",
              "      <td>Can I play with Jack?</td>\n",
              "      <td>cdc 5 (can I play with Jack).wav</td>\n",
              "      <td>CONTROLS_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>cdc 6 (the polar bears are dancing).mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.32</td>\n",
              "      <td>CONTROLS</td>\n",
              "      <td>0.0</td>\n",
              "      <td>the polar bears are dancing</td>\n",
              "      <td>Um, the polar bears are dancing.</td>\n",
              "      <td>cdc 6 (the polar bears are dancing).wav</td>\n",
              "      <td>CONTROLS_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ACPA ted had a dog with white feet-3.mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.13</td>\n",
              "      <td>CASES</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ted had a dog with white feet</td>\n",
              "      <td>Ted and a dog with white feet.</td>\n",
              "      <td>ACPA ted had a dog with white feet-3.wav</td>\n",
              "      <td>CASES_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>Video 1_4 (seesaw).mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.15</td>\n",
              "      <td>CASES</td>\n",
              "      <td>1.0</td>\n",
              "      <td>seesaw</td>\n",
              "      <td>P.S.A.</td>\n",
              "      <td>Video 1_4 (seesaw).wav</td>\n",
              "      <td>CASES_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>Video 4_4 (well it will help me).mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.32</td>\n",
              "      <td>CASES</td>\n",
              "      <td>1.0</td>\n",
              "      <td>well it will help me</td>\n",
              "      <td>Wow, em vá»«a há»c Ä‘Ä©a</td>\n",
              "      <td>Video 4_4 (well it will help me).wav</td>\n",
              "      <td>CASES_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>ACPA buy baby a bib.mp3</td>\n",
              "      <td>48000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.92</td>\n",
              "      <td>CASES</td>\n",
              "      <td>1.0</td>\n",
              "      <td>buy baby a bib</td>\n",
              "      <td>Hi, I'm Hayley Mim.</td>\n",
              "      <td>ACPA buy baby a bib.wav</td>\n",
              "      <td>CASES_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Video 1_18 (pretend it stops running when the ...</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.80</td>\n",
              "      <td>CONTROLS</td>\n",
              "      <td>0.0</td>\n",
              "      <td>pretend it stops running when the car is going</td>\n",
              "      <td>When it stops running, when the car is going.</td>\n",
              "      <td>Video 1_18 (pretend it stops running when the ...</td>\n",
              "      <td>CONTROLS_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>Video 2_4 (daddy).mp3</td>\n",
              "      <td>44100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.57</td>\n",
              "      <td>CASES</td>\n",
              "      <td>1.0</td>\n",
              "      <td>daddy</td>\n",
              "      <td>Fanny</td>\n",
              "      <td>Video 2_4 (daddy).wav</td>\n",
              "      <td>CASES_WAV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>ACPA we shouldn_t play in the street-2.mp3</td>\n",
              "      <td>48000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.16</td>\n",
              "      <td>CASES</td>\n",
              "      <td>1.0</td>\n",
              "      <td>we shouldn't play in the street</td>\n",
              "      <td>I shouldn't play with food.</td>\n",
              "      <td>ACPA we shouldn_t play in the street-2.wav</td>\n",
              "      <td>CASES_WAV</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>102 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             File_Name  Sampling_Rate_(Hz)  \\\n",
              "93        ACPA Santa came home since the snow fell.mp3             44100.0   \n",
              "140                   cdc 5 (can I play with Jack).mp3             44100.0   \n",
              "108            cdc 6 (the polar bears are dancing).mp3             44100.0   \n",
              "0             ACPA ted had a dog with white feet-3.mp3             44100.0   \n",
              "73                              Video 1_4 (seesaw).mp3             44100.0   \n",
              "..                                                 ...                 ...   \n",
              "71                Video 4_4 (well it will help me).mp3             44100.0   \n",
              "106                            ACPA buy baby a bib.mp3             48000.0   \n",
              "14   Video 1_18 (pretend it stops running when the ...             44100.0   \n",
              "92                               Video 2_4 (daddy).mp3             44100.0   \n",
              "102         ACPA we shouldn_t play in the street-2.mp3             48000.0   \n",
              "\n",
              "     Channels  Duration_(seconds)    folder  hypernasality  \\\n",
              "93        1.0                3.19  CONTROLS            0.0   \n",
              "140       2.0                1.57  CONTROLS            0.0   \n",
              "108       2.0                2.32  CONTROLS            0.0   \n",
              "0         1.0                4.13     CASES            1.0   \n",
              "73        2.0                1.15     CASES            1.0   \n",
              "..        ...                 ...       ...            ...   \n",
              "71        2.0                2.32     CASES            1.0   \n",
              "106       1.0                1.92     CASES            1.0   \n",
              "14        2.0                5.80  CONTROLS            0.0   \n",
              "92        2.0                0.57     CASES            1.0   \n",
              "102       1.0                2.16     CASES            1.0   \n",
              "\n",
              "                                      original_text  \\\n",
              "93              Santa came home since the snow fell   \n",
              "140                            can I play with Jack   \n",
              "108                     the polar bears are dancing   \n",
              "0                     ted had a dog with white feet   \n",
              "73                                           seesaw   \n",
              "..                                              ...   \n",
              "71                             well it will help me   \n",
              "106                                  buy baby a bib   \n",
              "14   pretend it stops running when the car is going   \n",
              "92                                            daddy   \n",
              "102                 we shouldn't play in the street   \n",
              "\n",
              "                                OPENAI_Whisper_text  \\\n",
              "93            Santa came home since the snow fell.    \n",
              "140                          Can I play with Jack?    \n",
              "108               Um, the polar bears are dancing.    \n",
              "0                   Ted and a dog with white feet.    \n",
              "73                                          P.S.A.    \n",
              "..                                              ...   \n",
              "71                             Wow, em vá»«a há»c Ä‘Ä©a    \n",
              "106                            Hi, I'm Hayley Mim.    \n",
              "14   When it stops running, when the car is going.    \n",
              "92                                           Fanny    \n",
              "102                    I shouldn't play with food.    \n",
              "\n",
              "                                          WAV_filename    WAV_folder  \n",
              "93        ACPA Santa came home since the snow fell.wav  CONTROLS_WAV  \n",
              "140                   cdc 5 (can I play with Jack).wav  CONTROLS_WAV  \n",
              "108            cdc 6 (the polar bears are dancing).wav  CONTROLS_WAV  \n",
              "0             ACPA ted had a dog with white feet-3.wav     CASES_WAV  \n",
              "73                              Video 1_4 (seesaw).wav     CASES_WAV  \n",
              "..                                                 ...           ...  \n",
              "71                Video 4_4 (well it will help me).wav     CASES_WAV  \n",
              "106                            ACPA buy baby a bib.wav     CASES_WAV  \n",
              "14   Video 1_18 (pretend it stops running when the ...  CONTROLS_WAV  \n",
              "92                               Video 2_4 (daddy).wav     CASES_WAV  \n",
              "102         ACPA we shouldn_t play in the street-2.wav     CASES_WAV  \n",
              "\n",
              "[102 rows x 10 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvLayklhYpSe"
      },
      "source": [
        "Making a dummy label set to make sure that my model isn't taking random guesses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez1PM8qwNDsC"
      },
      "outputs": [],
      "source": [
        "# dummy data\n",
        "import random\n",
        "\n",
        "# Define the length of the list you want\n",
        "length = len(train_labels)  # Change this to your desired length\n",
        "\n",
        "# Generate a list of random 1s and 0s of the specified length\n",
        "dummy_list = [random.choice([0, 1]) for _ in range(length)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATern9FEMwQF"
      },
      "outputs": [],
      "source": [
        "dummy_df = train_df\n",
        "dummy_df[\"DUMMY\"] = dummy_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEN4yr5zMiIG"
      },
      "outputs": [],
      "source": [
        "dummy_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
        "                                                  \"labels\":dummy_list}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "\n",
        "dummy_dataset = SpeechClassificationDataset(dummy_audio_dataset,  feature_extractor)\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oPd_WTlPiba"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"openai/whisper-base\"\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTW6vdGLMBsY",
        "outputId": "5cf25130-390a-4539-98b6-717d790f9ce1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "num_labels = 2\n",
        "\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds1BqzP5OuIE",
        "outputId": "d1abf7ef-6cbb-464f-c2f3-1a4487f81161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Batch 8/13, Train Loss: 0.7530\n",
            "========================================================================================\n",
            "Epoch 1/5, Val Loss: 0.7072, Val Accuracy: 0.4889, Val F1: 0.3284, Best Accuracy: 0.4889\n",
            "========================================================================================\n",
            "Epoch 2/5, Batch 8/13, Train Loss: 0.6759\n",
            "========================================================================================\n",
            "Epoch 2/5, Val Loss: 0.6932, Val Accuracy: 0.5333, Val F1: 0.5249, Best Accuracy: 0.5333\n",
            "========================================================================================\n",
            "Epoch 3/5, Batch 8/13, Train Loss: 0.2588\n",
            "========================================================================================\n",
            "Epoch 3/5, Val Loss: 1.1008, Val Accuracy: 0.4889, Val F1: 0.3631, Best Accuracy: 0.5333\n",
            "========================================================================================\n",
            "Epoch 4/5, Batch 8/13, Train Loss: 0.2797\n",
            "========================================================================================\n",
            "Epoch 4/5, Val Loss: 1.1925, Val Accuracy: 0.5333, Val F1: 0.4658, Best Accuracy: 0.5333\n",
            "========================================================================================\n",
            "Epoch 5/5, Batch 8/13, Train Loss: 0.0114\n",
            "========================================================================================\n",
            "Epoch 5/5, Val Loss: 1.7554, Val Accuracy: 0.5333, Val F1: 0.5181, Best Accuracy: 0.5333\n",
            "========================================================================================\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 5\n",
        "train(model, dummy_loader, val_loader, optimizer, criterion, device, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F40v4RGmRAoK"
      },
      "source": [
        "Model is not learning with the dummy data...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGSvxTq2VbAo"
      },
      "source": [
        "## Simpler Model\n",
        "\n",
        "Let's train a simpler model to see how our model does compared to a simpler one such as SVM or Random Forrest. Generated with help from ChatGPT4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoL6qAD3XPZa"
      },
      "source": [
        "### SVM\n",
        "\n",
        "Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNt0KvLKVhqC",
        "outputId": "851e631a-3fb4-4e0f-8257-dd2c1f1a381c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8333333333333334\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.83      0.86        18\n",
            "         1.0       0.77      0.83      0.80        12\n",
            "\n",
            "    accuracy                           0.83        30\n",
            "   macro avg       0.83      0.83      0.83        30\n",
            "weighted avg       0.84      0.83      0.83        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Function to extract Mel-Frequency Cepstral Coefficients (MFCCs) from an audio file.\n",
        "def extract_mfcc_features(file_path, n_mfcc=13):\n",
        "    # Load the audio file using librosa.\n",
        "    audio, sample_rate = librosa.load(file_path, sr=None)\n",
        "    # Extract MFCC features from the audio.\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "    # Scale the MFCCs by taking the average across time, resulting in a fixed-size vector per audio file.\n",
        "    mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
        "    return mfccs_scaled\n",
        "\n",
        "# Combine audio file paths from both training and testing datasets.\n",
        "audio_files = train_full_paths + test_full_paths\n",
        "# Combine the corresponding labels from both training and testing datasets.\n",
        "labels = train_labels + test_labels\n",
        "\n",
        "# Extract MFCC features from each audio file.\n",
        "features = [extract_mfcc_features(file) for file in audio_files]\n",
        "\n",
        "# Split the dataset into training, testing, and validation sets.\n",
        "X_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features by removing the mean and scaling to unit variance.\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(x_train)\n",
        "X_test = scaler.transform(x_test)\n",
        "\n",
        "# Initialize and train the Support Vector Machine (SVM) classifier with a linear kernel.\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions using the validation set.\n",
        "y_pred = svm_model.predict(x_val)\n",
        "\n",
        "# Evaluate the model's performance on the validation set.\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
        "print(\"Classification Report:\", classification_report(y_val, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gch3m5CvXTBW"
      },
      "source": [
        "### Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtACXySZXU2d",
        "outputId": "5b5fa04a-6fd2-4bb0-f056-e3a62d1854ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8333333333333334\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.83      0.86        18\n",
            "         1.0       0.77      0.83      0.80        12\n",
            "\n",
            "    accuracy                           0.83        30\n",
            "   macro avg       0.83      0.83      0.83        30\n",
            "weighted avg       0.84      0.83      0.83        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize and train the Random Forest classifier.\n",
        "# 'RandomForestClassifier' is a type of ensemble learning method, where multiple decision trees are used.\n",
        "# 'n_estimators=100' specifies that 100 trees should be used in the forest.\n",
        "# This parameter can be adjusted to optimize performance.\n",
        "rf_model = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Training the Random Forest model with the training data.\n",
        "# 'x_train' contains the feature vectors, and 'y_train' contains the corresponding labels.\n",
        "rf_model.fit(x_train, y_train)\n",
        "\n",
        "# Making predictions using the trained Random Forest model on the validation dataset.\n",
        "# 'x_val' contains the feature vectors of the validation set.\n",
        "y_pred = rf_model.predict(x_val)\n",
        "\n",
        "# Evaluating the performance of the Random Forest classifier.\n",
        "# 'accuracy_score' measures the overall accuracy of the model on the validation set.\n",
        "# 'classification_report' provides a detailed report including metrics like precision, recall, and F1-score for each class.\n",
        "# These metrics are helpful to understand the model's performance in detail, especially in multi-class classification tasks.\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
        "print(\"Classification Report:\", classification_report(y_val, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
