{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyB0fe7yF4Mh"
      },
      "source": [
        "# Fine-tuning Whisper on Speech Pathology Dataset\n",
        "\n",
        "\n",
        "\n",
        "## Goal\n",
        "\n",
        "The goal of the Cleft Palate project  at Vanderbilt DSI is to classify audio clips of patients' voices as containing hypernasality (a speech impediment) or not. The patients with hypernasality can then be recommended for speech pathology intervention. This is currently evaluated by human speech pathologists, which requires access to these medical providers. Our hope is to train a model that can classify this speech impediment for expedited patient access to a speech pathologist.\n",
        "\n",
        "Tutorial created with guidance from [\"Fine Tuning OpenAI Whisper Model for Audio Classifcation in PyTorch\"](https://www.daniweb.com/programming/computer-science/tutorials/540802/fine-tuning-openai-whisper-model-for-audio-classification-in-pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-jy3au5F4Mi"
      },
      "source": [
        "## Model\n",
        "\n",
        "We plan to use the Whisper embedings from OpenAI and train a classification model, either using Whisper with a sequence classification head or another classification LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZrXyRlPF4Mi"
      },
      "source": [
        "## Data\n",
        "\n",
        "The data in this notebook is publicly available voice recordings featuring hypernasality and control groups. In the future we hope to train our model on private patient data from Vanderbilt University Medical Center (VUMC)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0frbPwrsF4Mj",
        "outputId": "52f99284-8956-45ba-d8c4-f8f5d128110e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0a0+81ea7a4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (12.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.24.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.1.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.57.1+1.g4157f3379)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.8.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.40.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.11.17)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.39.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install datasets\n",
        "!pip install librosa\n",
        "!pip install transformers\n",
        "\n",
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "import random\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# import libraries\n",
        "import datasets\n",
        "from datasets import load_dataset, DatasetDict,  Audio\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "from transformers import WhisperModel, WhisperFeatureExtractor, AdamW\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnutJj9-R5bk"
      },
      "outputs": [],
      "source": [
        "data_path = \"/workspace/cleft_palate_choja/WAV_PUBLIC_SAMPLES/NOISE\"\n",
        "\n",
        "train_catalog = \"/workspace/cleft_palate_choja/train_noise.csv\"\n",
        "test_catalog = \"/workspace/cleft_palate_choja/test_noise.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "j7aCAhsoUwCx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNfrvzfXR5bl"
      },
      "source": [
        "# ==================================================================================================================\n",
        "# Tiny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UNczqVkMCGU",
        "outputId": "dbd4c351-063e-4d9d-9b41-229232120a00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1944/3453226131.py:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================\n",
            "Training\n",
            "\n",
            "Epoch 1/5, Batch 8/41, Train Loss: 0.7262\n",
            "Epoch 1/5, Batch 16/41, Train Loss: 0.5654\n",
            "Epoch 1/5, Batch 24/41, Train Loss: 0.4667\n",
            "Epoch 1/5, Batch 32/41, Train Loss: 0.9074\n",
            "Epoch 1/5, Batch 40/41, Train Loss: 0.4750\n",
            "========================================================================================\n",
            "Epoch 1/5, Val Loss: 0.2922, Val Accuracy: 0.9438, Val F1: 0.9428, Best Accuracy: 0.9438\n",
            "========================================================================================\n",
            "Epoch 2/5, Batch 8/41, Train Loss: 0.2944\n",
            "Epoch 2/5, Batch 16/41, Train Loss: 0.2388\n",
            "Epoch 2/5, Batch 24/41, Train Loss: 0.2523\n",
            "Epoch 2/5, Batch 32/41, Train Loss: 0.0043\n",
            "Epoch 2/5, Batch 40/41, Train Loss: 0.0159\n",
            "========================================================================================\n",
            "Epoch 2/5, Val Loss: 0.0795, Val Accuracy: 0.9775, Val F1: 0.9775, Best Accuracy: 0.9775\n",
            "========================================================================================\n",
            "Epoch 3/5, Batch 8/41, Train Loss: 0.0017\n",
            "Epoch 3/5, Batch 16/41, Train Loss: 0.0017\n",
            "Epoch 3/5, Batch 24/41, Train Loss: 0.0025\n",
            "Epoch 3/5, Batch 32/41, Train Loss: 0.4147\n",
            "Epoch 3/5, Batch 40/41, Train Loss: 0.0087\n",
            "========================================================================================\n",
            "Epoch 3/5, Val Loss: 0.4051, Val Accuracy: 0.8876, Val F1: 0.8875, Best Accuracy: 0.9775\n",
            "========================================================================================\n",
            "Epoch 4/5, Batch 8/41, Train Loss: 0.0042\n",
            "Epoch 4/5, Batch 16/41, Train Loss: 0.0290\n",
            "Epoch 4/5, Batch 24/41, Train Loss: 0.7116\n",
            "Epoch 4/5, Batch 32/41, Train Loss: 0.0201\n",
            "Epoch 4/5, Batch 40/41, Train Loss: 0.0119\n",
            "========================================================================================\n",
            "Epoch 4/5, Val Loss: 0.0190, Val Accuracy: 1.0000, Val F1: 1.0000, Best Accuracy: 1.0000\n",
            "========================================================================================\n",
            "Epoch 5/5, Batch 8/41, Train Loss: 0.0023\n",
            "Epoch 5/5, Batch 16/41, Train Loss: 0.0009\n",
            "Epoch 5/5, Batch 24/41, Train Loss: 0.0006\n",
            "Epoch 5/5, Batch 32/41, Train Loss: 0.0013\n",
            "Epoch 5/5, Batch 40/41, Train Loss: 0.0001\n",
            "========================================================================================\n",
            "Epoch 5/5, Val Loss: 0.0072, Val Accuracy: 1.0000, Val F1: 1.0000, Best Accuracy: 1.0000\n",
            "========================================================================================\n",
            "\n",
            "\n",
            "\n",
            "================\n",
            "Validtation\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        41\n",
            "           1       1.00      1.00      1.00        48\n",
            "\n",
            "    accuracy                           1.00        89\n",
            "   macro avg       1.00      1.00      1.00        89\n",
            "weighted avg       1.00      1.00      1.00        89\n",
            "\n",
            "1.0\n",
            "================\n",
            "Testing\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.78      0.88        36\n",
            "           1       0.83      1.00      0.90        38\n",
            "\n",
            "    accuracy                           0.89        74\n",
            "   macro avg       0.91      0.89      0.89        74\n",
            "weighted avg       0.91      0.89      0.89        74\n",
            "\n",
            "0.8918918918918919\n"
          ]
        }
      ],
      "source": [
        "model_checkpoint = \"openai/whisper-tiny\"\n",
        "\n",
        "train_metadata = pd.read_csv(train_catalog)\n",
        "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)\n",
        "train_files = train_df[\"WAV_filename\"].tolist()\n",
        "train_folder = train_df[\"WAV_folder\"].tolist()\n",
        "train_full_paths = [os.path.join(data_path,train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
        "train_labels = train_df[\"hypernasality\"].tolist()\n",
        "# val set\n",
        "val_files = val_df[\"WAV_filename\"].tolist()\n",
        "\n",
        "val_folder = val_df[\"WAV_folder\"].tolist()\n",
        "\n",
        "val_full_paths = [os.path.join(data_path,val_folder[i], val_files[i]) for i in range(0,len(val_files))]\n",
        "\n",
        "val_labels = val_df[\"hypernasality\"].tolist()\n",
        "\n",
        "test_metadata = pd.read_csv(test_catalog)\n",
        "# add cols for wav data\n",
        "\n",
        "# Replace \".mp3\" with \".wav\" in the \"Filename\" column\n",
        "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
        "\n",
        "# Create \"WAV_folder\" column by concatenating \"_WAV\" to the \"folder\" column\n",
        "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\"\n",
        "\n",
        "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
        "\n",
        "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
        "\n",
        "test_full_paths = [os.path.join(data_path,test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
        "\n",
        "#test_full_paths\n",
        "\n",
        "test_labels = test_metadata[\"hypernasality\"].tolist()\n",
        "\n",
        "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
        "                                                  \"labels\":train_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths,\n",
        "                                                  \"labels\": test_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths,\n",
        "                                                 \"labels\": val_labels }\n",
        "                                             ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "#model_checkpoint = \"openai/whisper-base\"\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_data,  text_processor):\n",
        "        self.audio_data = audio_data\n",
        "        self.text_processor = text_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
        "      input_features = inputs.input_features\n",
        "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
        "\n",
        "      labels = np.array(self.audio_data[index]['labels'])\n",
        "\n",
        "      return input_features, decoder_input_ids, torch.tensor(labels)\n",
        "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
        "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
        "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class SpeechClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, encoder):\n",
        "        super(SpeechClassifier, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_features, decoder_input_ids):\n",
        "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
        "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "num_labels = 2\n",
        "\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define the training function NO VAL\n",
        "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "          loss = criterion(logits, labels)\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 8 == 0:\n",
        "              print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
        "\n",
        "    torch.save(model.state_dict(), 'best_model.pt')\n",
        "# Define the training function\n",
        "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
        "    best_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "            input_features = input_features.squeeze()\n",
        "            input_features = input_features.to(device)\n",
        "            decoder_input_ids = decoder_input_ids.squeeze()\n",
        "            decoder_input_ids = decoder_input_ids.to(device)\n",
        "            labels = labels.view(-1)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i+1) % 8 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
        "                train_loss = 0.0\n",
        "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(\"========================================================================================\")\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
        "        print(\"========================================================================================\")\n",
        "def evaluate(model, data_loader,  device):\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "          loss = criterion(logits, labels)\n",
        "          total_loss += loss.item()\n",
        "          _, preds = torch.max(logits, 1)\n",
        "          all_labels.append(labels.cpu().numpy())\n",
        "          all_preds.append(preds.cpu().numpy())\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return loss, accuracy, f1, all_labels, all_preds\n",
        "\n",
        "#TRAINING\n",
        "print(\"================\")\n",
        "print(\"Training\\n\")\n",
        "\n",
        "import librosa\n",
        "num_epochs = 5\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\n",
        "\n",
        "\n",
        "\n",
        "#VALIDATION\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)\n",
        "\n",
        "#VALIDATION\n",
        "print(\"\\n\\n\")\n",
        "print(\"================\")\n",
        "print(\"Validtation\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))\n",
        "\n",
        "# TESTING ONLY\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"================\")\n",
        "print(\"Testing\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#==================================================================================================================\n",
        "# Base"
      ],
      "metadata": {
        "id": "aHsdNniJCBCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"openai/whisper-base\"\n",
        "\n",
        "train_metadata = pd.read_csv(train_catalog)\n",
        "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)\n",
        "train_files = train_df[\"WAV_filename\"].tolist()\n",
        "train_folder = train_df[\"WAV_folder\"].tolist()\n",
        "train_full_paths = [os.path.join(data_path,train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
        "train_labels = train_df[\"hypernasality\"].tolist()\n",
        "# val set\n",
        "val_files = val_df[\"WAV_filename\"].tolist()\n",
        "\n",
        "val_folder = val_df[\"WAV_folder\"].tolist()\n",
        "\n",
        "val_full_paths = [os.path.join(data_path,val_folder[i], val_files[i]) for i in range(0,len(val_files))]\n",
        "\n",
        "val_labels = val_df[\"hypernasality\"].tolist()\n",
        "\n",
        "test_metadata = pd.read_csv(test_catalog)\n",
        "# add cols for wav data\n",
        "\n",
        "# Replace \".mp3\" with \".wav\" in the \"Filename\" column\n",
        "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
        "\n",
        "# Create \"WAV_folder\" column by concatenating \"_WAV\" to the \"folder\" column\n",
        "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\"\n",
        "\n",
        "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
        "\n",
        "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
        "\n",
        "test_full_paths = [os.path.join(data_path,test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
        "\n",
        "#test_full_paths\n",
        "\n",
        "test_labels = test_metadata[\"hypernasality\"].tolist()\n",
        "\n",
        "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
        "                                                  \"labels\":train_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths,\n",
        "                                                  \"labels\": test_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths,\n",
        "                                                 \"labels\": val_labels }\n",
        "                                             ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "#model_checkpoint = \"openai/whisper-base\"\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_data,  text_processor):\n",
        "        self.audio_data = audio_data\n",
        "        self.text_processor = text_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
        "      input_features = inputs.input_features\n",
        "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
        "\n",
        "      labels = np.array(self.audio_data[index]['labels'])\n",
        "\n",
        "      return input_features, decoder_input_ids, torch.tensor(labels)\n",
        "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
        "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
        "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class SpeechClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, encoder):\n",
        "        super(SpeechClassifier, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_features, decoder_input_ids):\n",
        "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
        "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "num_labels = 2\n",
        "\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define the training function NO VAL\n",
        "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "          loss = criterion(logits, labels)\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 8 == 0:\n",
        "              print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
        "\n",
        "    torch.save(model.state_dict(), 'best_model.pt')\n",
        "# Define the training function\n",
        "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
        "    best_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "            input_features = input_features.squeeze()\n",
        "            input_features = input_features.to(device)\n",
        "            decoder_input_ids = decoder_input_ids.squeeze()\n",
        "            decoder_input_ids = decoder_input_ids.to(device)\n",
        "            labels = labels.view(-1)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i+1) % 8 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
        "                train_loss = 0.0\n",
        "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(\"========================================================================================\")\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
        "        print(\"========================================================================================\")\n",
        "def evaluate(model, data_loader,  device):\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "          loss = criterion(logits, labels)\n",
        "          total_loss += loss.item()\n",
        "          _, preds = torch.max(logits, 1)\n",
        "          all_labels.append(labels.cpu().numpy())\n",
        "          all_preds.append(preds.cpu().numpy())\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return loss, accuracy, f1, all_labels, all_preds\n",
        "\n",
        "#TRAINING\n",
        "print(\"================\")\n",
        "print(\"Training\\n\")\n",
        "\n",
        "import librosa\n",
        "num_epochs = 5\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\n",
        "\n",
        "\n",
        "\n",
        "#VALIDATION\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)\n",
        "\n",
        "#VALIDATION\n",
        "print(\"\\n\\n\")\n",
        "print(\"================\")\n",
        "print(\"Validtation\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))\n",
        "\n",
        "# TESTING ONLY\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"================\")\n",
        "print(\"Testing\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))"
      ],
      "metadata": {
        "id": "2cmgk-o2B-RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c61eN7d5R5bn"
      },
      "source": [
        "# ==================================================================================================================\n",
        "# Small"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vFN_6KtfB-_L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGZRqKNsR5bo",
        "outputId": "5bd38424-5729-4334-f6d8-206a5f8639aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1944/2574032072.py:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================\n",
            "Training\n",
            "\n",
            "Epoch 1/5, Batch 8/41, Train Loss: 0.7032\n",
            "Epoch 1/5, Batch 16/41, Train Loss: 0.3631\n",
            "Epoch 1/5, Batch 24/41, Train Loss: 0.1418\n",
            "Epoch 1/5, Batch 32/41, Train Loss: 0.4466\n",
            "Epoch 1/5, Batch 40/41, Train Loss: 0.5679\n",
            "========================================================================================\n",
            "Epoch 1/5, Val Loss: 0.2577, Val Accuracy: 0.8764, Val F1: 0.8762, Best Accuracy: 0.8764\n",
            "========================================================================================\n",
            "Epoch 2/5, Batch 8/41, Train Loss: 0.0354\n",
            "Epoch 2/5, Batch 16/41, Train Loss: 0.0261\n",
            "Epoch 2/5, Batch 24/41, Train Loss: 0.0035\n",
            "Epoch 2/5, Batch 32/41, Train Loss: 0.0321\n",
            "Epoch 2/5, Batch 40/41, Train Loss: 0.0224\n",
            "========================================================================================\n",
            "Epoch 2/5, Val Loss: 0.3240, Val Accuracy: 0.9101, Val F1: 0.9101, Best Accuracy: 0.9101\n",
            "========================================================================================\n",
            "Epoch 3/5, Batch 8/41, Train Loss: 0.0639\n",
            "Epoch 3/5, Batch 16/41, Train Loss: 0.0118\n",
            "Epoch 3/5, Batch 24/41, Train Loss: 0.0061\n",
            "Epoch 3/5, Batch 32/41, Train Loss: 0.0101\n",
            "Epoch 3/5, Batch 40/41, Train Loss: 0.0044\n",
            "========================================================================================\n",
            "Epoch 3/5, Val Loss: 0.0147, Val Accuracy: 0.9888, Val F1: 0.9887, Best Accuracy: 0.9888\n",
            "========================================================================================\n",
            "Epoch 4/5, Batch 8/41, Train Loss: 0.0010\n",
            "Epoch 4/5, Batch 16/41, Train Loss: 0.0006\n",
            "Epoch 4/5, Batch 24/41, Train Loss: 0.0011\n",
            "Epoch 4/5, Batch 32/41, Train Loss: 0.0007\n",
            "Epoch 4/5, Batch 40/41, Train Loss: 0.0001\n",
            "========================================================================================\n",
            "Epoch 4/5, Val Loss: 0.0021, Val Accuracy: 1.0000, Val F1: 1.0000, Best Accuracy: 1.0000\n",
            "========================================================================================\n",
            "Epoch 5/5, Batch 8/41, Train Loss: 0.0004\n",
            "Epoch 5/5, Batch 16/41, Train Loss: 0.0007\n",
            "Epoch 5/5, Batch 24/41, Train Loss: 0.0000\n",
            "Epoch 5/5, Batch 32/41, Train Loss: 0.0001\n",
            "Epoch 5/5, Batch 40/41, Train Loss: 0.0000\n",
            "========================================================================================\n",
            "Epoch 5/5, Val Loss: 0.0013, Val Accuracy: 1.0000, Val F1: 1.0000, Best Accuracy: 1.0000\n",
            "========================================================================================\n",
            "\n",
            "\n",
            "\n",
            "================\n",
            "Validtation\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        41\n",
            "           1       1.00      1.00      1.00        48\n",
            "\n",
            "    accuracy                           1.00        89\n",
            "   macro avg       1.00      1.00      1.00        89\n",
            "weighted avg       1.00      1.00      1.00        89\n",
            "\n",
            "1.0\n",
            "================\n",
            "Testing\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.94      0.89        36\n",
            "           1       0.94      0.84      0.89        38\n",
            "\n",
            "    accuracy                           0.89        74\n",
            "   macro avg       0.90      0.89      0.89        74\n",
            "weighted avg       0.90      0.89      0.89        74\n",
            "\n",
            "0.8918918918918919\n"
          ]
        }
      ],
      "source": [
        "model_checkpoint = \"openai/whisper-small\"\n",
        "\n",
        "train_metadata = pd.read_csv(train_catalog)\n",
        "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)\n",
        "train_files = train_df[\"WAV_filename\"].tolist()\n",
        "train_folder = train_df[\"WAV_folder\"].tolist()\n",
        "train_full_paths = [os.path.join(data_path,train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
        "train_labels = train_df[\"hypernasality\"].tolist()\n",
        "# val set\n",
        "val_files = val_df[\"WAV_filename\"].tolist()\n",
        "\n",
        "val_folder = val_df[\"WAV_folder\"].tolist()\n",
        "\n",
        "val_full_paths = [os.path.join(data_path,val_folder[i], val_files[i]) for i in range(0,len(val_files))]\n",
        "\n",
        "val_labels = val_df[\"hypernasality\"].tolist()\n",
        "\n",
        "test_metadata = pd.read_csv(test_catalog)\n",
        "# add cols for wav data\n",
        "\n",
        "# Replace \".mp3\" with \".wav\" in the \"Filename\" column\n",
        "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
        "\n",
        "# Create \"WAV_folder\" column by concatenating \"_WAV\" to the \"folder\" column\n",
        "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\"\n",
        "\n",
        "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
        "\n",
        "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
        "\n",
        "test_full_paths = [os.path.join(data_path,test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
        "\n",
        "#test_full_paths\n",
        "\n",
        "test_labels = test_metadata[\"hypernasality\"].tolist()\n",
        "\n",
        "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
        "                                                  \"labels\":train_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths,\n",
        "                                                  \"labels\": test_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths,\n",
        "                                                 \"labels\": val_labels }\n",
        "                                             ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "#model_checkpoint = \"openai/whisper-base\"\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_data,  text_processor):\n",
        "        self.audio_data = audio_data\n",
        "        self.text_processor = text_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
        "      input_features = inputs.input_features\n",
        "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
        "\n",
        "      labels = np.array(self.audio_data[index]['labels'])\n",
        "\n",
        "      return input_features, decoder_input_ids, torch.tensor(labels)\n",
        "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
        "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
        "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class SpeechClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, encoder):\n",
        "        super(SpeechClassifier, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_features, decoder_input_ids):\n",
        "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
        "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "num_labels = 2\n",
        "\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define the training function NO VAL\n",
        "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "          loss = criterion(logits, labels)\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 8 == 0:\n",
        "              print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
        "\n",
        "    torch.save(model.state_dict(), 'best_model.pt')\n",
        "# Define the training function\n",
        "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
        "    best_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "            input_features = input_features.squeeze()\n",
        "            input_features = input_features.to(device)\n",
        "            decoder_input_ids = decoder_input_ids.squeeze()\n",
        "            decoder_input_ids = decoder_input_ids.to(device)\n",
        "            labels = labels.view(-1)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i+1) % 8 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
        "                train_loss = 0.0\n",
        "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(\"========================================================================================\")\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
        "        print(\"========================================================================================\")\n",
        "def evaluate(model, data_loader,  device):\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "          loss = criterion(logits, labels)\n",
        "          total_loss += loss.item()\n",
        "          _, preds = torch.max(logits, 1)\n",
        "          all_labels.append(labels.cpu().numpy())\n",
        "          all_preds.append(preds.cpu().numpy())\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return loss, accuracy, f1, all_labels, all_preds\n",
        "\n",
        "#TRAINING\n",
        "print(\"================\")\n",
        "print(\"Training\\n\")\n",
        "\n",
        "import librosa\n",
        "num_epochs = 5\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\n",
        "\n",
        "\n",
        "\n",
        "#VALIDATION\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)\n",
        "\n",
        "#VALIDATION\n",
        "print(\"\\n\\n\")\n",
        "print(\"================\")\n",
        "print(\"Validtation\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))\n",
        "\n",
        "# TESTING ONLY\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"================\")\n",
        "print(\"Testing\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNFgRXK5R5bp"
      },
      "source": [
        "# ==================================================================================================================\n",
        "# Medium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxbdCZ1-R5bp",
        "outputId": "891637c6-ee73-4b92-ecf0-dfe502612d0c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1944/739343374.py:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================\n",
            "Training\n",
            "\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 100.62 MiB is free. Process 3637514 has 7.23 GiB memory in use. Process 3669360 has 4.69 GiB memory in use. Process 3747169 has 51.01 GiB memory in use. Process 3770862 has 16.09 GiB memory in use. Of the allocated memory 15.06 GiB is allocated by PyTorch, and 524.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 205\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[1;32m    204\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m--> 205\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m#VALIDATION\u001b[39;00m\n\u001b[1;32m    210\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[7], line 157\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m    155\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    156\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 157\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m    159\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[7], line 99\u001b[0m, in \u001b[0;36mSpeechClassifier.forward\u001b[0;34m(self, input_features, decoder_input_ids)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_features, decoder_input_ids):\n\u001b[0;32m---> 99\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m][:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m    101\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(pooled_output)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:1612\u001b[0m, in \u001b[0;36mWhisperModel.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1610\u001b[0m     input_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_input_features(input_features, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m-> 1612\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:1215\u001b[0m, in \u001b[0;36mWhisperEncoder.forward\u001b[0;34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1207\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1208\u001b[0m             encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1209\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1212\u001b[0m             output_attentions,\n\u001b[1;32m   1213\u001b[0m         )\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1215\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1222\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:782\u001b[0m, in \u001b[0;36mWhisperEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    780\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    781\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(hidden_states)\n\u001b[0;32m--> 782\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_dropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    784\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_states)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 100.62 MiB is free. Process 3637514 has 7.23 GiB memory in use. Process 3669360 has 4.69 GiB memory in use. Process 3747169 has 51.01 GiB memory in use. Process 3770862 has 16.09 GiB memory in use. Of the allocated memory 15.06 GiB is allocated by PyTorch, and 524.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "model_checkpoint = \"openai/whisper-medium\"\n",
        "\n",
        "train_metadata = pd.read_csv(train_catalog)\n",
        "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)\n",
        "train_files = train_df[\"WAV_filename\"].tolist()\n",
        "train_folder = train_df[\"WAV_folder\"].tolist()\n",
        "train_full_paths = [os.path.join(data_path,train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
        "train_labels = train_df[\"hypernasality\"].tolist()\n",
        "# val set\n",
        "val_files = val_df[\"WAV_filename\"].tolist()\n",
        "\n",
        "val_folder = val_df[\"WAV_folder\"].tolist()\n",
        "\n",
        "val_full_paths = [os.path.join(data_path,val_folder[i], val_files[i]) for i in range(0,len(val_files))]\n",
        "\n",
        "val_labels = val_df[\"hypernasality\"].tolist()\n",
        "\n",
        "test_metadata = pd.read_csv(test_catalog)\n",
        "# add cols for wav data\n",
        "\n",
        "# Replace \".mp3\" with \".wav\" in the \"Filename\" column\n",
        "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
        "\n",
        "# Create \"WAV_folder\" column by concatenating \"_WAV\" to the \"folder\" column\n",
        "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\"\n",
        "\n",
        "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
        "\n",
        "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
        "\n",
        "test_full_paths = [os.path.join(data_path,test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
        "\n",
        "#test_full_paths\n",
        "\n",
        "test_labels = test_metadata[\"hypernasality\"].tolist()\n",
        "\n",
        "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
        "                                                  \"labels\":train_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths,\n",
        "                                                  \"labels\": test_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths,\n",
        "                                                 \"labels\": val_labels }\n",
        "                                             ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "#model_checkpoint = \"openai/whisper-base\"\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_data,  text_processor):\n",
        "        self.audio_data = audio_data\n",
        "        self.text_processor = text_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
        "      input_features = inputs.input_features\n",
        "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
        "\n",
        "      labels = np.array(self.audio_data[index]['labels'])\n",
        "\n",
        "      return input_features, decoder_input_ids, torch.tensor(labels)\n",
        "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
        "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
        "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class SpeechClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, encoder):\n",
        "        super(SpeechClassifier, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_features, decoder_input_ids):\n",
        "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
        "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "num_labels = 2\n",
        "\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define the training function NO VAL\n",
        "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "          loss = criterion(logits, labels)\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 8 == 0:\n",
        "              print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
        "\n",
        "    torch.save(model.state_dict(), 'best_model.pt')\n",
        "# Define the training function\n",
        "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
        "    best_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "            input_features = input_features.squeeze()\n",
        "            input_features = input_features.to(device)\n",
        "            decoder_input_ids = decoder_input_ids.squeeze()\n",
        "            decoder_input_ids = decoder_input_ids.to(device)\n",
        "            labels = labels.view(-1)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i+1) % 8 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
        "                train_loss = 0.0\n",
        "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(\"========================================================================================\")\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
        "        print(\"========================================================================================\")\n",
        "def evaluate(model, data_loader,  device):\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "          loss = criterion(logits, labels)\n",
        "          total_loss += loss.item()\n",
        "          _, preds = torch.max(logits, 1)\n",
        "          all_labels.append(labels.cpu().numpy())\n",
        "          all_preds.append(preds.cpu().numpy())\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return loss, accuracy, f1, all_labels, all_preds\n",
        "\n",
        "#TRAINING\n",
        "print(\"================\")\n",
        "print(\"Training\\n\")\n",
        "\n",
        "import librosa\n",
        "num_epochs = 5\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\n",
        "\n",
        "\n",
        "\n",
        "#VALIDATION\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)\n",
        "\n",
        "#VALIDATION\n",
        "print(\"\\n\\n\")\n",
        "print(\"================\")\n",
        "print(\"Validtation\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))\n",
        "\n",
        "# TESTING ONLY\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"================\")\n",
        "print(\"Testing\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rly_KZpR5bq"
      },
      "source": [
        "# ==================================================================================================================\n",
        "# Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06NIuY4KR5br"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"openai/whisper-large-v2\"\n",
        "\n",
        "train_metadata = pd.read_csv(train_catalog)\n",
        "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)\n",
        "train_files = train_df[\"WAV_filename\"].tolist()\n",
        "train_folder = train_df[\"WAV_folder\"].tolist()\n",
        "train_full_paths = [os.path.join(data_path,train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
        "train_labels = train_df[\"hypernasality\"].tolist()\n",
        "# val set\n",
        "val_files = val_df[\"WAV_filename\"].tolist()\n",
        "\n",
        "val_folder = val_df[\"WAV_folder\"].tolist()\n",
        "\n",
        "val_full_paths = [os.path.join(data_path,val_folder[i], val_files[i]) for i in range(0,len(val_files))]\n",
        "\n",
        "val_labels = val_df[\"hypernasality\"].tolist()\n",
        "\n",
        "test_metadata = pd.read_csv(test_catalog)\n",
        "# add cols for wav data\n",
        "\n",
        "# Replace \".mp3\" with \".wav\" in the \"Filename\" column\n",
        "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
        "\n",
        "# Create \"WAV_folder\" column by concatenating \"_WAV\" to the \"folder\" column\n",
        "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\"\n",
        "\n",
        "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
        "\n",
        "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
        "\n",
        "test_full_paths = [os.path.join(data_path,test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
        "\n",
        "#test_full_paths\n",
        "\n",
        "test_labels = test_metadata[\"hypernasality\"].tolist()\n",
        "\n",
        "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
        "                                                  \"labels\":train_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths,\n",
        "                                                  \"labels\": test_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths,\n",
        "                                                 \"labels\": val_labels }\n",
        "                                             ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "#model_checkpoint = \"openai/whisper-base\"\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_data,  text_processor):\n",
        "        self.audio_data = audio_data\n",
        "        self.text_processor = text_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
        "      input_features = inputs.input_features\n",
        "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
        "\n",
        "      labels = np.array(self.audio_data[index]['labels'])\n",
        "\n",
        "      return input_features, decoder_input_ids, torch.tensor(labels)\n",
        "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
        "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
        "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class SpeechClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, encoder):\n",
        "        super(SpeechClassifier, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_features, decoder_input_ids):\n",
        "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
        "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "num_labels = 2\n",
        "\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define the training function NO VAL\n",
        "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "          loss = criterion(logits, labels)\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 8 == 0:\n",
        "              print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
        "\n",
        "    torch.save(model.state_dict(), 'best_model.pt')\n",
        "# Define the training function\n",
        "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
        "    best_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "            input_features = input_features.squeeze()\n",
        "            input_features = input_features.to(device)\n",
        "            decoder_input_ids = decoder_input_ids.squeeze()\n",
        "            decoder_input_ids = decoder_input_ids.to(device)\n",
        "            labels = labels.view(-1)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i+1) % 8 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
        "                train_loss = 0.0\n",
        "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(\"========================================================================================\")\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
        "        print(\"========================================================================================\")\n",
        "def evaluate(model, data_loader,  device):\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "          loss = criterion(logits, labels)\n",
        "          total_loss += loss.item()\n",
        "          _, preds = torch.max(logits, 1)\n",
        "          all_labels.append(labels.cpu().numpy())\n",
        "          all_preds.append(preds.cpu().numpy())\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return loss, accuracy, f1, all_labels, all_preds\n",
        "\n",
        "#TRAINING\n",
        "print(\"================\")\n",
        "print(\"Training\\n\")\n",
        "\n",
        "import librosa\n",
        "num_epochs = 5\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\n",
        "\n",
        "\n",
        "\n",
        "#VALIDATION\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)\n",
        "\n",
        "#VALIDATION\n",
        "print(\"\\n\\n\")\n",
        "print(\"================\")\n",
        "print(\"Validtation\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))\n",
        "\n",
        "# TESTING ONLY\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"================\")\n",
        "print(\"Testing\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_pX8oFTR5br"
      },
      "source": [
        "# SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX1ZajD1R5bs",
        "outputId": "d7bd5a18-afbf-4fae-9a9e-f6063b338d70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7796610169491526\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.87      0.80        30\n",
            "         1.0       0.83      0.69      0.75        29\n",
            "\n",
            "    accuracy                           0.78        59\n",
            "   macro avg       0.79      0.78      0.78        59\n",
            "weighted avg       0.79      0.78      0.78        59\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Define a function to extract MFCCs from an audio file\n",
        "def extract_mfcc_features(file_path, n_mfcc=13):\n",
        "    audio, sample_rate = librosa.load(file_path, sr=None)\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "    mfccs_scaled = np.mean(mfccs.T, axis=0)  # Taking the average across time\n",
        "    return mfccs_scaled\n",
        "\n",
        "# Paths to your audio files (replace these with your actual file paths)\n",
        "audio_files = train_full_paths + test_full_paths  # Add more paths as needed\n",
        "labels = train_labels + test_labels  # Corresponding labels for your audio files\n",
        "\n",
        "# Extract features from each audio file\n",
        "features = [extract_mfcc_features(file) for file in audio_files]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features by removing the mean and scaling to unit variance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(x_train)\n",
        "X_test = scaler.transform(x_test)\n",
        "\n",
        "# Initialize and train the SVM classifier\n",
        "svm_model = SVC(kernel='linear')  # You can experiment with different kernels\n",
        "svm_model.fit(x_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = svm_model.predict(x_val)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
        "print(\"Classification Report:\", classification_report(y_val, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qrO9fU4R5bs"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-at7uiO5R5bs",
        "outputId": "07ea961a-2b85-4d65-86b9-868ed01f0ec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.864406779661017\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.87      0.87        30\n",
            "         1.0       0.86      0.86      0.86        29\n",
            "\n",
            "    accuracy                           0.86        59\n",
            "   macro avg       0.86      0.86      0.86        59\n",
            "weighted avg       0.86      0.86      0.86        59\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Initialize and train the Random Forest classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100)  # You can adjust the number of trees\n",
        "rf_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions - VAL\n",
        "y_pred = rf_model.predict(x_val)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
        "print(\"Classification Report:\", classification_report(y_val, y_pred))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}