{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyB0fe7yF4Mh"
      },
      "source": [
        "# Fine-tuning Whisper on Speech Pathology Dataset\n",
        "\n",
        "## Goal\n",
        "\n",
        "The goal of the Cleft Palate project at Vanderbilt DSI is to classify audio clips of patients' voices as containing hypernasality (a speech impediment) or not. The patients with hypernasality can then be recommended for speech pathology intervention. This is currently evaluated by human speech pathologists, which requires access to these medical providers. Our hope is to train a model that can classify this speech impediment for expedited patient access to a speech pathologist.\n",
        "\n",
        "Tutorial created with guidance from [\"Fine Tuning OpenAI Whisper Model for Audio Classifcation in PyTorch\"](https://www.daniweb.com/programming/computer-science/tutorials/540802/fine-tuning-openai-whisper-model-for-audio-classification-in-pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-jy3au5F4Mi"
      },
      "source": [
        "## Model\n",
        "\n",
        "We plan to use the Whisper embedings from OpenAI and train a classification model, either using Whisper with a sequence classification head or another classification LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZrXyRlPF4Mi"
      },
      "source": [
        "## Data\n",
        "\n",
        "The data in this notebook is publicly available voice recordings featuring hypernasality and control groups. In the future we hope to train our model on private patient data from Vanderbilt University Medical Center (VUMC)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0frbPwrsF4Mj",
        "outputId": "bdd02929-10bd-4118-d724-ea1bde413299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0a0+81ea7a4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (12.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.24.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.1.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.57.1+1.g4157f3379)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.8.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.40.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.11.17)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.39.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install datasets\n",
        "!pip install librosa\n",
        "!pip install transformers\n",
        "# import libraries\n",
        "import datasets\n",
        "from datasets import load_dataset, DatasetDict,  Audio\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "from transformers import WhisperModel, WhisperFeatureExtractor, AdamW\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVLNh9pYQh6h"
      },
      "outputs": [],
      "source": [
        "data_path = \"/workspace/cleft_palate_choja/WAV_PUBLIC_SAMPLES/NOISE\"\n",
        "\n",
        "train_catalog = \"/workspace/cleft_palate_choja/train_noise.csv\"\n",
        "test_catalog = \"/workspace/cleft_palate_choja/test_noise.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyna_eCuQh6h"
      },
      "source": [
        "# ==================================================================================================================\n",
        "# Tiny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UNczqVkMCGU",
        "outputId": "7446f02f-d6d2-4232-ca01-2dfcaab607d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1944/3453226131.py:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================\n",
            "Training\n",
            "\n",
            "Epoch 1/5, Batch 8/41, Train Loss: 0.7262\n",
            "Epoch 1/5, Batch 16/41, Train Loss: 0.5654\n",
            "Epoch 1/5, Batch 24/41, Train Loss: 0.4667\n",
            "Epoch 1/5, Batch 32/41, Train Loss: 0.9074\n",
            "Epoch 1/5, Batch 40/41, Train Loss: 0.4750\n",
            "========================================================================================\n",
            "Epoch 1/5, Val Loss: 0.2922, Val Accuracy: 0.9438, Val F1: 0.9428, Best Accuracy: 0.9438\n",
            "========================================================================================\n",
            "Epoch 2/5, Batch 8/41, Train Loss: 0.2944\n",
            "Epoch 2/5, Batch 16/41, Train Loss: 0.2388\n",
            "Epoch 2/5, Batch 24/41, Train Loss: 0.2523\n",
            "Epoch 2/5, Batch 32/41, Train Loss: 0.0043\n",
            "Epoch 2/5, Batch 40/41, Train Loss: 0.0159\n",
            "========================================================================================\n",
            "Epoch 2/5, Val Loss: 0.0795, Val Accuracy: 0.9775, Val F1: 0.9775, Best Accuracy: 0.9775\n",
            "========================================================================================\n",
            "Epoch 3/5, Batch 8/41, Train Loss: 0.0017\n",
            "Epoch 3/5, Batch 16/41, Train Loss: 0.0017\n",
            "Epoch 3/5, Batch 24/41, Train Loss: 0.0025\n",
            "Epoch 3/5, Batch 32/41, Train Loss: 0.4147\n",
            "Epoch 3/5, Batch 40/41, Train Loss: 0.0087\n",
            "========================================================================================\n",
            "Epoch 3/5, Val Loss: 0.4051, Val Accuracy: 0.8876, Val F1: 0.8875, Best Accuracy: 0.9775\n",
            "========================================================================================\n",
            "Epoch 4/5, Batch 8/41, Train Loss: 0.0042\n",
            "Epoch 4/5, Batch 16/41, Train Loss: 0.0290\n",
            "Epoch 4/5, Batch 24/41, Train Loss: 0.7116\n",
            "Epoch 4/5, Batch 32/41, Train Loss: 0.0201\n",
            "Epoch 4/5, Batch 40/41, Train Loss: 0.0119\n",
            "========================================================================================\n",
            "Epoch 4/5, Val Loss: 0.0190, Val Accuracy: 1.0000, Val F1: 1.0000, Best Accuracy: 1.0000\n",
            "========================================================================================\n",
            "Epoch 5/5, Batch 8/41, Train Loss: 0.0023\n",
            "Epoch 5/5, Batch 16/41, Train Loss: 0.0009\n",
            "Epoch 5/5, Batch 24/41, Train Loss: 0.0006\n",
            "Epoch 5/5, Batch 32/41, Train Loss: 0.0013\n",
            "Epoch 5/5, Batch 40/41, Train Loss: 0.0001\n",
            "========================================================================================\n",
            "Epoch 5/5, Val Loss: 0.0072, Val Accuracy: 1.0000, Val F1: 1.0000, Best Accuracy: 1.0000\n",
            "========================================================================================\n",
            "\n",
            "\n",
            "\n",
            "================\n",
            "Validtation\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        41\n",
            "           1       1.00      1.00      1.00        48\n",
            "\n",
            "    accuracy                           1.00        89\n",
            "   macro avg       1.00      1.00      1.00        89\n",
            "weighted avg       1.00      1.00      1.00        89\n",
            "\n",
            "1.0\n",
            "================\n",
            "Testing\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.78      0.88        36\n",
            "           1       0.83      1.00      0.90        38\n",
            "\n",
            "    accuracy                           0.89        74\n",
            "   macro avg       0.91      0.89      0.89        74\n",
            "weighted avg       0.91      0.89      0.89        74\n",
            "\n",
            "0.8918918918918919\n"
          ]
        }
      ],
      "source": [
        "model_checkpoint = \"openai/whisper-tiny\"\n",
        "\n",
        "train_metadata = pd.read_csv(train_catalog)\n",
        "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)\n",
        "train_files = train_df[\"WAV_filename\"].tolist()\n",
        "train_folder = train_df[\"WAV_folder\"].tolist()\n",
        "train_full_paths = [os.path.join(data_path,train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
        "train_labels = train_df[\"hypernasality\"].tolist()\n",
        "# val set\n",
        "val_files = val_df[\"WAV_filename\"].tolist()\n",
        "\n",
        "val_folder = val_df[\"WAV_folder\"].tolist()\n",
        "\n",
        "val_full_paths = [os.path.join(data_path,val_folder[i], val_files[i]) for i in range(0,len(val_files))]\n",
        "\n",
        "val_labels = val_df[\"hypernasality\"].tolist()\n",
        "\n",
        "test_metadata = pd.read_csv(test_catalog)\n",
        "# add cols for wav data\n",
        "\n",
        "# Replace \".mp3\" with \".wav\" in the \"Filename\" column\n",
        "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
        "\n",
        "# Create \"WAV_folder\" column by concatenating \"_WAV\" to the \"folder\" column\n",
        "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\"\n",
        "\n",
        "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
        "\n",
        "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
        "\n",
        "test_full_paths = [os.path.join(data_path,test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
        "\n",
        "#test_full_paths\n",
        "\n",
        "test_labels = test_metadata[\"hypernasality\"].tolist()\n",
        "\n",
        "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
        "                                                  \"labels\":train_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths,\n",
        "                                                  \"labels\": test_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths,\n",
        "                                                 \"labels\": val_labels }\n",
        "                                             ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "#model_checkpoint = \"openai/whisper-base\"\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_data,  text_processor):\n",
        "        self.audio_data = audio_data\n",
        "        self.text_processor = text_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
        "      input_features = inputs.input_features\n",
        "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
        "\n",
        "      labels = np.array(self.audio_data[index]['labels'])\n",
        "\n",
        "      return input_features, decoder_input_ids, torch.tensor(labels)\n",
        "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
        "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
        "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class SpeechClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, encoder):\n",
        "        super(SpeechClassifier, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_features, decoder_input_ids):\n",
        "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
        "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "num_labels = 2\n",
        "\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define the training function NO VAL\n",
        "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "          loss = criterion(logits, labels)\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 8 == 0:\n",
        "              print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
        "\n",
        "    torch.save(model.state_dict(), 'best_model.pt')\n",
        "# Define the training function\n",
        "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
        "    best_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "            input_features = input_features.squeeze()\n",
        "            input_features = input_features.to(device)\n",
        "            decoder_input_ids = decoder_input_ids.squeeze()\n",
        "            decoder_input_ids = decoder_input_ids.to(device)\n",
        "            labels = labels.view(-1)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i+1) % 8 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
        "                train_loss = 0.0\n",
        "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(\"========================================================================================\")\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
        "        print(\"========================================================================================\")\n",
        "def evaluate(model, data_loader,  device):\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "          loss = criterion(logits, labels)\n",
        "          total_loss += loss.item()\n",
        "          _, preds = torch.max(logits, 1)\n",
        "          all_labels.append(labels.cpu().numpy())\n",
        "          all_preds.append(preds.cpu().numpy())\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return loss, accuracy, f1, all_labels, all_preds\n",
        "\n",
        "#TRAINING\n",
        "print(\"================\")\n",
        "print(\"Training\\n\")\n",
        "\n",
        "import librosa\n",
        "num_epochs = 5\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\n",
        "\n",
        "\n",
        "\n",
        "#VALIDATION\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)\n",
        "\n",
        "#VALIDATION\n",
        "print(\"\\n\\n\")\n",
        "print(\"================\")\n",
        "print(\"Validtation\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))\n",
        "\n",
        "# TESTING ONLY\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"================\")\n",
        "print(\"Testing\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak9EJNMKQh6i"
      },
      "source": [
        "# ==================================================================================================================\n",
        "# Small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRYE3iVFQh6i",
        "outputId": "8d6ac16a-6b63-43d9-eb69-ad9dcff60d2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1944/2574032072.py:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================\n",
            "Training\n",
            "\n",
            "Epoch 1/5, Batch 8/41, Train Loss: 0.7032\n",
            "Epoch 1/5, Batch 16/41, Train Loss: 0.3631\n",
            "Epoch 1/5, Batch 24/41, Train Loss: 0.1418\n",
            "Epoch 1/5, Batch 32/41, Train Loss: 0.4466\n",
            "Epoch 1/5, Batch 40/41, Train Loss: 0.5679\n",
            "========================================================================================\n",
            "Epoch 1/5, Val Loss: 0.2577, Val Accuracy: 0.8764, Val F1: 0.8762, Best Accuracy: 0.8764\n",
            "========================================================================================\n",
            "Epoch 2/5, Batch 8/41, Train Loss: 0.0354\n",
            "Epoch 2/5, Batch 16/41, Train Loss: 0.0261\n",
            "Epoch 2/5, Batch 24/41, Train Loss: 0.0035\n",
            "Epoch 2/5, Batch 32/41, Train Loss: 0.0321\n",
            "Epoch 2/5, Batch 40/41, Train Loss: 0.0224\n",
            "========================================================================================\n",
            "Epoch 2/5, Val Loss: 0.3240, Val Accuracy: 0.9101, Val F1: 0.9101, Best Accuracy: 0.9101\n",
            "========================================================================================\n",
            "Epoch 3/5, Batch 8/41, Train Loss: 0.0639\n",
            "Epoch 3/5, Batch 16/41, Train Loss: 0.0118\n",
            "Epoch 3/5, Batch 24/41, Train Loss: 0.0061\n",
            "Epoch 3/5, Batch 32/41, Train Loss: 0.0101\n",
            "Epoch 3/5, Batch 40/41, Train Loss: 0.0044\n",
            "========================================================================================\n",
            "Epoch 3/5, Val Loss: 0.0147, Val Accuracy: 0.9888, Val F1: 0.9887, Best Accuracy: 0.9888\n",
            "========================================================================================\n",
            "Epoch 4/5, Batch 8/41, Train Loss: 0.0010\n",
            "Epoch 4/5, Batch 16/41, Train Loss: 0.0006\n",
            "Epoch 4/5, Batch 24/41, Train Loss: 0.0011\n",
            "Epoch 4/5, Batch 32/41, Train Loss: 0.0007\n",
            "Epoch 4/5, Batch 40/41, Train Loss: 0.0001\n",
            "========================================================================================\n",
            "Epoch 4/5, Val Loss: 0.0021, Val Accuracy: 1.0000, Val F1: 1.0000, Best Accuracy: 1.0000\n",
            "========================================================================================\n",
            "Epoch 5/5, Batch 8/41, Train Loss: 0.0004\n",
            "Epoch 5/5, Batch 16/41, Train Loss: 0.0007\n",
            "Epoch 5/5, Batch 24/41, Train Loss: 0.0000\n",
            "Epoch 5/5, Batch 32/41, Train Loss: 0.0001\n",
            "Epoch 5/5, Batch 40/41, Train Loss: 0.0000\n",
            "========================================================================================\n",
            "Epoch 5/5, Val Loss: 0.0013, Val Accuracy: 1.0000, Val F1: 1.0000, Best Accuracy: 1.0000\n",
            "========================================================================================\n",
            "\n",
            "\n",
            "\n",
            "================\n",
            "Validtation\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        41\n",
            "           1       1.00      1.00      1.00        48\n",
            "\n",
            "    accuracy                           1.00        89\n",
            "   macro avg       1.00      1.00      1.00        89\n",
            "weighted avg       1.00      1.00      1.00        89\n",
            "\n",
            "1.0\n",
            "================\n",
            "Testing\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.94      0.89        36\n",
            "           1       0.94      0.84      0.89        38\n",
            "\n",
            "    accuracy                           0.89        74\n",
            "   macro avg       0.90      0.89      0.89        74\n",
            "weighted avg       0.90      0.89      0.89        74\n",
            "\n",
            "0.8918918918918919\n"
          ]
        }
      ],
      "source": [
        "model_checkpoint = \"openai/whisper-small\"\n",
        "\n",
        "train_metadata = pd.read_csv(train_catalog)\n",
        "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)\n",
        "train_files = train_df[\"WAV_filename\"].tolist()\n",
        "train_folder = train_df[\"WAV_folder\"].tolist()\n",
        "train_full_paths = [os.path.join(data_path,train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
        "train_labels = train_df[\"hypernasality\"].tolist()\n",
        "# val set\n",
        "val_files = val_df[\"WAV_filename\"].tolist()\n",
        "\n",
        "val_folder = val_df[\"WAV_folder\"].tolist()\n",
        "\n",
        "val_full_paths = [os.path.join(data_path,val_folder[i], val_files[i]) for i in range(0,len(val_files))]\n",
        "\n",
        "val_labels = val_df[\"hypernasality\"].tolist()\n",
        "\n",
        "test_metadata = pd.read_csv(test_catalog)\n",
        "# add cols for wav data\n",
        "\n",
        "# Replace \".mp3\" with \".wav\" in the \"Filename\" column\n",
        "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
        "\n",
        "# Create \"WAV_folder\" column by concatenating \"_WAV\" to the \"folder\" column\n",
        "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\"\n",
        "\n",
        "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
        "\n",
        "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
        "\n",
        "test_full_paths = [os.path.join(data_path,test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
        "\n",
        "#test_full_paths\n",
        "\n",
        "test_labels = test_metadata[\"hypernasality\"].tolist()\n",
        "\n",
        "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
        "                                                  \"labels\":train_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths,\n",
        "                                                  \"labels\": test_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths,\n",
        "                                                 \"labels\": val_labels }\n",
        "                                             ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "#model_checkpoint = \"openai/whisper-base\"\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_data,  text_processor):\n",
        "        self.audio_data = audio_data\n",
        "        self.text_processor = text_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
        "      input_features = inputs.input_features\n",
        "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
        "\n",
        "      labels = np.array(self.audio_data[index]['labels'])\n",
        "\n",
        "      return input_features, decoder_input_ids, torch.tensor(labels)\n",
        "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
        "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
        "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class SpeechClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, encoder):\n",
        "        super(SpeechClassifier, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_features, decoder_input_ids):\n",
        "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
        "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "num_labels = 2\n",
        "\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define the training function NO VAL\n",
        "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "          loss = criterion(logits, labels)\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 8 == 0:\n",
        "              print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
        "\n",
        "    torch.save(model.state_dict(), 'best_model.pt')\n",
        "# Define the training function\n",
        "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
        "    best_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "            input_features = input_features.squeeze()\n",
        "            input_features = input_features.to(device)\n",
        "            decoder_input_ids = decoder_input_ids.squeeze()\n",
        "            decoder_input_ids = decoder_input_ids.to(device)\n",
        "            labels = labels.view(-1)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i+1) % 8 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
        "                train_loss = 0.0\n",
        "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(\"========================================================================================\")\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
        "        print(\"========================================================================================\")\n",
        "def evaluate(model, data_loader,  device):\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "          loss = criterion(logits, labels)\n",
        "          total_loss += loss.item()\n",
        "          _, preds = torch.max(logits, 1)\n",
        "          all_labels.append(labels.cpu().numpy())\n",
        "          all_preds.append(preds.cpu().numpy())\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return loss, accuracy, f1, all_labels, all_preds\n",
        "\n",
        "#TRAINING\n",
        "print(\"================\")\n",
        "print(\"Training\\n\")\n",
        "\n",
        "import librosa\n",
        "num_epochs = 5\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\n",
        "\n",
        "\n",
        "\n",
        "#VALIDATION\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)\n",
        "\n",
        "#VALIDATION\n",
        "print(\"\\n\\n\")\n",
        "print(\"================\")\n",
        "print(\"Validtation\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))\n",
        "\n",
        "# TESTING ONLY\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"================\")\n",
        "print(\"Testing\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18bmRvMJQh6j"
      },
      "source": [
        "# ==================================================================================================================\n",
        "# Medium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9rP5OW8Qh6j",
        "outputId": "f9cb6c1e-8561-4fba-b3e8-f6d510c57067"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1944/739343374.py:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================\n",
            "Training\n",
            "\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 100.62 MiB is free. Process 3637514 has 7.23 GiB memory in use. Process 3669360 has 4.69 GiB memory in use. Process 3747169 has 51.01 GiB memory in use. Process 3770862 has 16.09 GiB memory in use. Of the allocated memory 15.06 GiB is allocated by PyTorch, and 524.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 205\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[1;32m    204\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m--> 205\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m#VALIDATION\u001b[39;00m\n\u001b[1;32m    210\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[7], line 157\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m    155\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    156\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 157\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m    159\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[7], line 99\u001b[0m, in \u001b[0;36mSpeechClassifier.forward\u001b[0;34m(self, input_features, decoder_input_ids)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_features, decoder_input_ids):\n\u001b[0;32m---> 99\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m][:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m    101\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(pooled_output)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:1612\u001b[0m, in \u001b[0;36mWhisperModel.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1610\u001b[0m     input_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_input_features(input_features, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m-> 1612\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:1215\u001b[0m, in \u001b[0;36mWhisperEncoder.forward\u001b[0;34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1207\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1208\u001b[0m             encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1209\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1212\u001b[0m             output_attentions,\n\u001b[1;32m   1213\u001b[0m         )\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1215\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1222\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:782\u001b[0m, in \u001b[0;36mWhisperEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    780\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    781\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(hidden_states)\n\u001b[0;32m--> 782\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_dropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    784\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_states)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 100.62 MiB is free. Process 3637514 has 7.23 GiB memory in use. Process 3669360 has 4.69 GiB memory in use. Process 3747169 has 51.01 GiB memory in use. Process 3770862 has 16.09 GiB memory in use. Of the allocated memory 15.06 GiB is allocated by PyTorch, and 524.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "model_checkpoint = \"openai/whisper-medium\"\n",
        "\n",
        "train_metadata = pd.read_csv(train_catalog)\n",
        "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)\n",
        "train_files = train_df[\"WAV_filename\"].tolist()\n",
        "train_folder = train_df[\"WAV_folder\"].tolist()\n",
        "train_full_paths = [os.path.join(data_path,train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
        "train_labels = train_df[\"hypernasality\"].tolist()\n",
        "# val set\n",
        "val_files = val_df[\"WAV_filename\"].tolist()\n",
        "\n",
        "val_folder = val_df[\"WAV_folder\"].tolist()\n",
        "\n",
        "val_full_paths = [os.path.join(data_path,val_folder[i], val_files[i]) for i in range(0,len(val_files))]\n",
        "\n",
        "val_labels = val_df[\"hypernasality\"].tolist()\n",
        "\n",
        "test_metadata = pd.read_csv(test_catalog)\n",
        "# add cols for wav data\n",
        "\n",
        "# Replace \".mp3\" with \".wav\" in the \"Filename\" column\n",
        "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
        "\n",
        "# Create \"WAV_folder\" column by concatenating \"_WAV\" to the \"folder\" column\n",
        "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\"\n",
        "\n",
        "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
        "\n",
        "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
        "\n",
        "test_full_paths = [os.path.join(data_path,test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
        "\n",
        "#test_full_paths\n",
        "\n",
        "test_labels = test_metadata[\"hypernasality\"].tolist()\n",
        "\n",
        "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
        "                                                  \"labels\":train_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths,\n",
        "                                                  \"labels\": test_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths,\n",
        "                                                 \"labels\": val_labels }\n",
        "                                             ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "#model_checkpoint = \"openai/whisper-base\"\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_data,  text_processor):\n",
        "        self.audio_data = audio_data\n",
        "        self.text_processor = text_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
        "      input_features = inputs.input_features\n",
        "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
        "\n",
        "      labels = np.array(self.audio_data[index]['labels'])\n",
        "\n",
        "      return input_features, decoder_input_ids, torch.tensor(labels)\n",
        "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
        "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
        "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class SpeechClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, encoder):\n",
        "        super(SpeechClassifier, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_features, decoder_input_ids):\n",
        "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
        "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "num_labels = 2\n",
        "\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define the training function NO VAL\n",
        "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "          loss = criterion(logits, labels)\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 8 == 0:\n",
        "              print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
        "\n",
        "    torch.save(model.state_dict(), 'best_model.pt')\n",
        "# Define the training function\n",
        "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
        "    best_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "            input_features = input_features.squeeze()\n",
        "            input_features = input_features.to(device)\n",
        "            decoder_input_ids = decoder_input_ids.squeeze()\n",
        "            decoder_input_ids = decoder_input_ids.to(device)\n",
        "            labels = labels.view(-1)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i+1) % 8 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
        "                train_loss = 0.0\n",
        "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(\"========================================================================================\")\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
        "        print(\"========================================================================================\")\n",
        "def evaluate(model, data_loader,  device):\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "          loss = criterion(logits, labels)\n",
        "          total_loss += loss.item()\n",
        "          _, preds = torch.max(logits, 1)\n",
        "          all_labels.append(labels.cpu().numpy())\n",
        "          all_preds.append(preds.cpu().numpy())\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return loss, accuracy, f1, all_labels, all_preds\n",
        "\n",
        "#TRAINING\n",
        "print(\"================\")\n",
        "print(\"Training\\n\")\n",
        "\n",
        "import librosa\n",
        "num_epochs = 5\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\n",
        "\n",
        "\n",
        "\n",
        "#VALIDATION\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)\n",
        "\n",
        "#VALIDATION\n",
        "print(\"\\n\\n\")\n",
        "print(\"================\")\n",
        "print(\"Validtation\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))\n",
        "\n",
        "# TESTING ONLY\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"================\")\n",
        "print(\"Testing\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PedD93UpQh6k"
      },
      "source": [
        "# ==================================================================================================================\n",
        "# Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6_P_h0dQh6k"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"openai/whisper-large-v2\"\n",
        "\n",
        "train_metadata = pd.read_csv(train_catalog)\n",
        "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)\n",
        "train_files = train_df[\"WAV_filename\"].tolist()\n",
        "train_folder = train_df[\"WAV_folder\"].tolist()\n",
        "train_full_paths = [os.path.join(data_path,train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
        "train_labels = train_df[\"hypernasality\"].tolist()\n",
        "# val set\n",
        "val_files = val_df[\"WAV_filename\"].tolist()\n",
        "\n",
        "val_folder = val_df[\"WAV_folder\"].tolist()\n",
        "\n",
        "val_full_paths = [os.path.join(data_path,val_folder[i], val_files[i]) for i in range(0,len(val_files))]\n",
        "\n",
        "val_labels = val_df[\"hypernasality\"].tolist()\n",
        "\n",
        "test_metadata = pd.read_csv(test_catalog)\n",
        "# add cols for wav data\n",
        "\n",
        "# Replace \".mp3\" with \".wav\" in the \"Filename\" column\n",
        "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
        "\n",
        "# Create \"WAV_folder\" column by concatenating \"_WAV\" to the \"folder\" column\n",
        "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\"\n",
        "\n",
        "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
        "\n",
        "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
        "\n",
        "test_full_paths = [os.path.join(data_path,test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
        "\n",
        "#test_full_paths\n",
        "\n",
        "test_labels = test_metadata[\"hypernasality\"].tolist()\n",
        "\n",
        "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
        "                                                  \"labels\":train_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths,\n",
        "                                                  \"labels\": test_labels}\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths,\n",
        "                                                 \"labels\": val_labels }\n",
        "                                             ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "#model_checkpoint = \"openai/whisper-base\"\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_data,  text_processor):\n",
        "        self.audio_data = audio_data\n",
        "        self.text_processor = text_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
        "      input_features = inputs.input_features\n",
        "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
        "\n",
        "      labels = np.array(self.audio_data[index]['labels'])\n",
        "\n",
        "      return input_features, decoder_input_ids, torch.tensor(labels)\n",
        "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
        "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
        "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class SpeechClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, encoder):\n",
        "        super(SpeechClassifier, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_features, decoder_input_ids):\n",
        "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
        "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "num_labels = 2\n",
        "\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define the training function NO VAL\n",
        "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "          loss = criterion(logits, labels)\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 8 == 0:\n",
        "              print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
        "\n",
        "    torch.save(model.state_dict(), 'best_model.pt')\n",
        "# Define the training function\n",
        "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
        "    best_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "            input_features = input_features.squeeze()\n",
        "            input_features = input_features.to(device)\n",
        "            decoder_input_ids = decoder_input_ids.squeeze()\n",
        "            decoder_input_ids = decoder_input_ids.to(device)\n",
        "            labels = labels.view(-1)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i+1) % 8 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
        "                train_loss = 0.0\n",
        "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(\"========================================================================================\")\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
        "        print(\"========================================================================================\")\n",
        "def evaluate(model, data_loader,  device):\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "          loss = criterion(logits, labels)\n",
        "          total_loss += loss.item()\n",
        "          _, preds = torch.max(logits, 1)\n",
        "          all_labels.append(labels.cpu().numpy())\n",
        "          all_preds.append(preds.cpu().numpy())\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return loss, accuracy, f1, all_labels, all_preds\n",
        "\n",
        "#TRAINING\n",
        "print(\"================\")\n",
        "print(\"Training\\n\")\n",
        "\n",
        "import librosa\n",
        "num_epochs = 5\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\n",
        "\n",
        "\n",
        "\n",
        "#VALIDATION\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)\n",
        "\n",
        "#VALIDATION\n",
        "print(\"\\n\\n\")\n",
        "print(\"================\")\n",
        "print(\"Validtation\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))\n",
        "\n",
        "# TESTING ONLY\n",
        "state_dict = torch.load('best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"================\")\n",
        "print(\"Testing\\n\\n\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgEDOHL5Qh6k"
      },
      "source": [
        "# SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R80Te2YcQh6k",
        "outputId": "ae86c69d-7771-4e1d-f41e-c27458d54ee3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7796610169491526\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.87      0.80        30\n",
            "         1.0       0.83      0.69      0.75        29\n",
            "\n",
            "    accuracy                           0.78        59\n",
            "   macro avg       0.79      0.78      0.78        59\n",
            "weighted avg       0.79      0.78      0.78        59\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Define a function to extract MFCCs from an audio file\n",
        "def extract_mfcc_features(file_path, n_mfcc=13):\n",
        "    audio, sample_rate = librosa.load(file_path, sr=None)\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "    mfccs_scaled = np.mean(mfccs.T, axis=0)  # Taking the average across time\n",
        "    return mfccs_scaled\n",
        "\n",
        "# Paths to your audio files (replace these with your actual file paths)\n",
        "audio_files = train_full_paths + test_full_paths  # Add more paths as needed\n",
        "labels = train_labels + test_labels  # Corresponding labels for your audio files\n",
        "\n",
        "# Extract features from each audio file\n",
        "features = [extract_mfcc_features(file) for file in audio_files]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features by removing the mean and scaling to unit variance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(x_train)\n",
        "X_test = scaler.transform(x_test)\n",
        "\n",
        "# Initialize and train the SVM classifier\n",
        "svm_model = SVC(kernel='linear')  # You can experiment with different kernels\n",
        "svm_model.fit(x_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = svm_model.predict(x_val)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
        "print(\"Classification Report:\", classification_report(y_val, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MyxDEV-Qh6k"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97pJYwuwQh6k",
        "outputId": "e054425b-26a1-4091-be4c-0ac0a9ef2c23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.864406779661017\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.87      0.87        30\n",
            "         1.0       0.86      0.86      0.86        29\n",
            "\n",
            "    accuracy                           0.86        59\n",
            "   macro avg       0.86      0.86      0.86        59\n",
            "weighted avg       0.86      0.86      0.86        59\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Initialize and train the Random Forest classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100)  # You can adjust the number of trees\n",
        "rf_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions - VAL\n",
        "y_pred = rf_model.predict(x_val)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
        "print(\"Classification Report:\", classification_report(y_val, y_pred))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}